# Daily Papers
The project automatically fetches the latest papers from arXiv based on keywords.

The subheadings in the README file represent the search keywords.

Only the most recent articles for each keyword are retained, up to a maximum of 100 papers.

You can click the 'Watch' button to receive daily email notifications.

Last update: 2025-11-12

## Code
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Codes induced by alternative codes](https://arxiv.org/pdf/1801.02598v1)** | 2018-01-09 | <details><summary>Show</summary><p>Alternative codes, an extension of the notion of ordinary codes, have been first introduced and considered by P. T. Huy et al. in 2004. As seen below, every alternative code, in its turn, defines an ordinary code. Such codes are called codes induced by alternative codes or alt-induced codes, for short. In this paper we consider these alt-induced codes and subclasses of them. In particular, characteristic properties of such codes are established, and an algorithm to check whether a finite code is alt-induced or not is proposed.</p></details> | <details><summary>16 pa...</summary><p>16 pages; Accepted for publication in AMV</p></details> |
| **[To Code, or Not To Code? Exploring Impact of Code in Pre-training](https://arxiv.org/pdf/2408.10914v1)** | 2024-08-21 | <details><summary>Show</summary><p>Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs' performance, there is only limited work analyzing the precise impact of code on non-code tasks. In this work, we systematically investigate the impact of code data on general performance. We ask "what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation". We conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, we find a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the addition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. Our work suggests investments in code quality and preserving code during pre-training have positive impacts.</p></details> |  |
| **[The Subfield Codes of Ovoid Codes](https://arxiv.org/pdf/1804.05516v1)** | 2018-04-17 | <details><summary>Show</summary><p>Ovoids in $\PG(3, \gf(q))$ have been an interesting topic in coding theory, combinatorics, and finite geometry for a long time. So far only two families of ovoids are known. The first is the elliptic quadratics and the second is the Tits ovoids. It is known that an ovoid in $\PG(3, \gf(q))$ corresponds to a $[q^2+1, 4, q^2-q]$ code over $\gf(q)$, which is called an ovoid code. The objectives of this paper is to study the subfield codes of the two families of ovoid codes. The dimensions, minimum weights, and the weight distributions of the subfield codes of the elliptic quadric codes and Tits ovoid codes are settled. The parameters of the duals of these subfield codes are also studied. Some of the codes presented in this paper are optimal, and some are distance-optimal. The parameters of the subfield codes are new.</p></details> |  |
| **[Monomial-Cartesian codes and their duals, with applications to LCD codes, quantum codes, and locally recoverable codes](https://arxiv.org/pdf/1907.11812v1)** | 2020-08-17 | <details><summary>Show</summary><p>A monomial-Cartesian code is an evaluation code defined by evaluating a set of monomials over a Cartesian product. It is a generalization of some families of codes in the literature, for instance toric codes, affine Cartesian codes and $J$-affine variety codes. In this work we use the vanishing ideal of the Cartesian product to give a description of the dual of a monomial-Cartesian code. Then we use such description of the dual to prove the existence of quantum error correcting codes and MDS quantum error correcting codes. Finally we show that the direct product of monomial-Cartesian codes is a locally recoverable code with $t$-availability if at least $t$ of the components are locally recoverable codes.</p></details> |  |
| **[The Extended Codes of Some Linear Codes](https://arxiv.org/pdf/2307.08053v2)** | 2023-12-05 | <details><summary>Show</summary><p>The classical way of extending an $[n, k, d]$ linear code $\C$ is to add an overall parity-check coordinate to each codeword of the linear code $\C$. This extended code, denoted by $\overline{\C}(-\bone)$ and called the standardly extended code of $\C$, is a linear code with parameters $[n+1, k, \bar{d}]$, where $\bar{d}=d$ or $\bar{d}=d+1$. This is one of the two extending techniques for linear codes in the literature. The standardly extended codes of some families of binary linear codes have been studied to some extent. However, not much is known about the standardly extended codes of nonbinary codes. For example, the minimum distances of the standardly extended codes of the nonbinary Hamming codes remain open for over 70 years. The first objective of this paper is to introduce the nonstandardly extended codes of a linear code and develop some general theory for this type of extended linear codes. The second objective is to study this type of extended codes of a number of families of linear codes, including cyclic codes and nonbinary Hamming codes. Four families of distance-optimal or dimension-optimal linear codes are obtained with this extending technique. The parameters of certain extended codes of many families of linear codes are settled in this paper.</p></details> |  |
| **[Construction of Parallel RIO Codes using Coset Coding with Hamming Codes](https://arxiv.org/pdf/1705.05596v1)** | 2018-12-26 | <details><summary>Show</summary><p>Random input/output (RIO) code is a coding scheme that enables reading of one logical page using a single read threshold in multilevel flash memory. The construction of RIO codes is equivalent to the construction of WOM codes. Parallel RIO (P-RIO) code is an RIO code that encodes all pages in parallel. In this paper, we utilize coset coding with Hamming codes in order to construct P-RIO codes. Coset coding is a technique that constructs WOM codes using linear binary codes. We leverage the information on the data of all pages to encode each page. Our constructed codes store more pages than RIO codes constructed via coset coding.</p></details> | 16 pages |
| **[List-decodable Codes and Covering Codes](https://arxiv.org/pdf/2109.02818v13)** | 2022-05-31 | <details><summary>Show</summary><p>The list-decodable code has been an active topic in theoretical computer science.There are general results about the list-decodability to the Johnson radius and the list-decoding capacity theorem. In this paper we show that rates, list-decodable radius and list sizes are closely related to the classical topic of covering codes. We prove new general simple but strong upper bounds for list-decodable codes in general finite metric spaces based on various covering codes. The general covering code upper bounds can be applied to the case that the volumes of the balls depend on the centers, not only on the radius. Then any good upper bound on the covering radius or the size of covering code imply a good upper bound on the sizes of list-decodable codes. Our results give exponential improvements on the recent generalized Singleton upper bound in STOC 2020 for Hamming metric list-decodable codes, when the code lengths are large. A generalized Singleton upper bound for average-radius list-decodable codes is also given from our general covering code upper bound. We also suggest to study the combinatorial covering list-decodable codes as a natural generalization of combinatorial list-decodable codes. We apply our general covering code upper bounds for list-decodable rank-metric codes, list-decodable subspace codes, list-decodable insertion codes list-decodable deletion codes,list-decodable sum-rank-metric codes and list decodable permutation codes. Some new better results about non-list-decodability of rank-metric codes, subspace codes, sum-rank-metric codes and permutation codes with various metrics are obtained.</p></details> | <details><summary>54 pa...</summary><p>54 pages, corrected version</p></details> |
| **[Perpetual Codes for Network Coding](https://arxiv.org/pdf/1509.04492v1)** | 2015-09-16 | <details><summary>Show</summary><p>Random Linear Network Coding (RLNC) provides a theoretically efficient method for coding. Some of its practical drawbacks are the complexity of decoding and the overhead due to the coding vectors. For computationally weak and battery-driven platforms, these challenges are particular important. In this work, we consider the coding variant Perpetual codes which are sparse, non-uniform and the coding vectors have a compact representation. The sparsity allows for fast encoding and decoding, and the non-uniform protection of symbols enables recoding where the produced symbols are indistinguishable from those encoded at the source. The presented results show that the approach can provide a coding overhead arbitrarily close to that of RLNC, but at reduced computational load. The achieved gain over RLNC grows with the generation size, and both encoding and decoding throughput is approximately one order of magnitude higher compared to RLNC at a generation size of 2048. Additionally, the approach allows for easy adjustment between coding throughput and code overhead, which makes it suitable for a broad range of platforms and applications.</p></details> | <details><summary>13 pa...</summary><p>13 pages, 9 figures, original draft from 2012 included in this phd thesis: http://vbn.aau.dk/files/123241894/Thesis_Low_computational_complexity_network_coding_for_mobile_networks_.pdf</p></details> |
| **[The Subfield Codes of Hyperoval and Conic codes](https://arxiv.org/pdf/1804.06003v1)** | 2018-04-18 | <details><summary>Show</summary><p>Hyperovals in $\PG(2,\gf(q))$ with even $q$ are maximal arcs and an interesting research topic in finite geometries and combinatorics. Hyperovals in $\PG(2,\gf(q))$ are equivalent to $[q+2,3,q]$ MDS codes over $\gf(q)$, called hyperoval codes, in the sense that one can be constructed from the other. Ovals in $\PG(2,\gf(q))$ for odd $q$ are equivalent to $[q+1,3,q-1]$ MDS codes over $\gf(q)$, which are called oval codes. In this paper, we investigate the binary subfield codes of two families of hyperoval codes and the $p$-ary subfield codes of the conic codes. The weight distributions of these subfield codes and the parameters of their duals are determined. As a byproduct, we generalize one family of the binary subfield codes to the $p$-ary case and obtain its weight distribution. The codes presented in this paper are optimal or almost optimal in many cases. In addition, the parameters of these binary codes and $p$-ary codes seem new.</p></details> |  |
| **[Abelian Group Codes for Source Coding and Channel Coding](https://arxiv.org/pdf/1305.1598v1)** | 2013-05-08 | <details><summary>Show</summary><p>In this paper, we study the asymptotic performance of Abelian group codes for the lossy source coding problem for arbitrary discrete (finite alphabet) memoryless sources as well as the channel coding problem for arbitrary discrete (finite alphabet) memoryless channels. For the source coding problem, we derive an achievable rate-distortion function that is characterized in a single-letter information-theoretic form using the ensemble of Abelian group codes. When the underlying group is a field, it simplifies to the symmetric rate-distortion function. Similarly, for the channel coding problem, we find an achievable rate characterized in a single-letter information-theoretic form using group codes. This simplifies to the symmetric capacity of the channel when the underlying group is a field. We compute the rate-distortion function and the achievable rate for several examples of sources and channels. Due to the non-symmetric nature of the sources and channels considered, our analysis uses a synergy of information theoretic and group-theoretic tools.</p></details> |  |
| **[Layered Subspace Codes for Network Coding](https://arxiv.org/pdf/1209.2894v1)** | 2012-09-14 | <details><summary>Show</summary><p>Subspace codes were introduced by Kötter and Kschischang for error control in random linear network coding. In this paper, a layered type of subspace codes is considered, which can be viewed as a superposition of multiple component subspace codes. Exploiting the layered structure, we develop two decoding algorithms for these codes. The first algorithm operates by separately decoding each component code. The second algorithm is similar to the successive interference cancellation (SIC) algorithm for conventional superposition coding, and further permits an iterative version. We show that both algorithms decode not only deterministically up to but also probabilistically beyond the error-correction capability of the overall code. Finally we present possible applications of layered subspace codes in several network coding scenarios.</p></details> | 13 pages, 6 figures |
| **[Fourier Codes and Hartley Codes](https://arxiv.org/pdf/1502.02489v1)** | 2015-02-10 | <details><summary>Show</summary><p>Real-valued block codes are introduced, which are derived from Discrete Fourier Transforms (DFT) and Discrete Hartley Transforms (DHT). These algebraic structures are built from the eigensequences of the transforms. Generator and parity check matrices were computed for codes up to block length N=24. They can be viewed as lattices codes so the main parameters (dimension, minimal norm, area of the Voronoi region, density, and centre density) are computed. Particularly, Hamming-Hartley and Golay-Hartley block codes are presented. These codes may possibly help an efficient computation of a DHT/DFT.</p></details> | <details><summary>5 pag...</summary><p>5 pages, 4 tables, 1 appedix. conference: XXV Simposio Brasileiro de Telecomunicacoes, SBrT'07, Recife, PE, Brazil, 2007</p></details> |
| **[Golden-Coded Index Coding](https://arxiv.org/pdf/1704.07014v1)** | 2017-04-25 | <details><summary>Show</summary><p>We study the problem of constructing good space-time codes for broadcasting $K$ independent messages over a MIMO network to $L$ users, where each user demands all the messages and already has a subset of messages as side information. As a first attempt, we consider the $2\times 2$ case and propose golden-coded index coding by partitioning the golden codes into $K$ subcodes, one for each message. The proposed scheme is shown to have the property that for any side information configuration, the minimum determinant of the code increases exponentially with the amount of information contained in the side information.</p></details> | <details><summary>5 pag...</summary><p>5 pages, 2 figures. Accepted for publication in 2017 IEEE ISIT</p></details> |
| **[On Code Rates of Fractional Repetition Codes](https://arxiv.org/pdf/1711.08869v1)** | 2017-11-27 | <details><summary>Show</summary><p>In \textit{Distributed Storage Systems} (DSSs), usually, data is stored using replicated packets on different chunk servers. Recently a new paradigm of \textit{Fractional Repetition} (FR) codes have been introduced, in which, data is replicated in a smart way on distributed servers using a \textit{Maximum Distance Separable} (MDS) code. In this work, for a non-uniform FR code, bounds on the FR code rate and DSS code rate are studied. Using matrix representation of an FR code, some universally good FR codes have been obtained.</p></details> | 11 pages, 0 figures |
| **[Extended codes and deep holes of MDS codes](https://arxiv.org/pdf/2312.05534v1)** | 2023-12-12 | <details><summary>Show</summary><p>For a given linear code $\C$ of length $n$ over $\gf(q)$ and a nonzero vector $\bu$ in $\gf(q)^n$, Sun, Ding and Chen defined an extended linear code $\overline{\C}(\bu)$ of $\C$, which is a generalisation of the classical extended code $\overline{\C}(-\bone)$ of $\C$ and called the second kind of an extended code of $\C$ (see arXiv:2307.04076 and arXiv:2307.08053). They developed some general theory of the extended codes $\overline{\C}(\bu)$ and studied the extended codes $\overline{\C}(\bu)$ of several families of linear codes, including cyclic codes, projective two-weight codes, nonbinary Hamming codes, and a family of reversible MDS cyclic codes. The objective of this paper is to investigate the extended codes $\overline{\C}(\bu)$ of MDS codes $\C$ over finite fields. The main result of this paper is that the extended code $\overline{\C}(\bu)$ of an MDS $[n,k]$ code $\C$ remains MDS if and only if the covering radius $ρ(\mathcal{C}^{\bot})=k$ and the vector $\bu$ is a deep hole of the dual code $\C^\perp$. As applications of this main result, the extended codes of the GRS codes and extended GRS codes are investigated and the covering radii of several families of MDS codes are determined.</p></details> | <details><summary>22 pa...</summary><p>22 pages, submitted for possible publication</p></details> |
| **[The Tap code - a code similar to Morse code for communication by tapping](https://arxiv.org/pdf/1304.5069v1)** | 2013-04-19 | <details><summary>Show</summary><p>A code is presented for fast, easy and efficient communication over channels that allow only two signal types: a single sound (e.g. a knock), or no sound (i.e. silence). This is a true binary code while Morse code is a ternary code and does not work in such situations. Thus the presented code is more universal than Morse and can be used in much more situations. Additionally it is very tolerant to variations in signal strength or duration. The paper contains various ways in which the code can be derived, that all lead to the same code. It also contains a comparison to other, similar codes, including the Morse code, in regards to efficiency and other attributes. The replacement of Morse code with Tap code is not proposed.</p></details> | 11 pages, 3 tables |
| **[The equivalence of GRS codes and EGRS codes](https://arxiv.org/pdf/2204.11960v1)** | 2022-04-27 | <details><summary>Show</summary><p>Generalized Reed-Solomon and extended generalized Reed-Solomon (abbreviation to GRS and EGRS) codes are the most well-known family of MDS codes with wide applications in coding theory and practice. Let $\mathbb{F}_q$ be the $q$ elements finite field, where $q$ is the power of a prime. For a linear code $\mathcal{C}$ over $\mathbb{F}_q$ with length $2\le n\le q$, we prove that $\mathcal{C}$ is a GRS code if and only if $\mathcal{C}$ is a EGRS code.</p></details> |  |
| **[Expansion Coding for Channel and Source Coding](https://arxiv.org/pdf/1505.05481v1)** | 2015-05-21 | <details><summary>Show</summary><p>A general method of coding over expansion is proposed,which allows one to reduce the highly non-trivial problems of coding over analog channels and compressing analog sources to a set of much simpler subproblems, coding over discrete channels and compressing discrete sources. More specifically, the focus of this paper is on the additive exponential noise (AEN) channel, and lossy compression of exponential sources. Taking advantage of the essential decomposable property of these channels (sources), the proposed expansion method allows for mapping of these problems to coding over parallel channels (respectively, sources), where each level is modeled as an independent coding problem over discrete alphabets. Any feasible solution to the resulting optimization problem after expansion corresponds to an achievable scheme of the original problem. Utilizing this mapping, even for the cases where the optimal solutions are difficult to characterize, it is shown that the expansion coding scheme still performs well with appropriate choices of parameters. More specifically, theoretical analysis and numerical results reveal that expansion coding achieves the capacity of AEN channel in the high SNR regime. It is also shown that for lossy compression, the achievable rate distortion pair by expansion coding approaches to the Shannon limit in the low distortion region. Remarkably, by using capacity-achieving codes with low encoding and decoding complexity that are originally designed for discrete alphabets, for instance polar codes, the proposed expansion coding scheme allows for designing low-complexity analog channel and source codes.</p></details> | 42 pages, 10 figures |
| **[Beyond Stabilizer Codes II: Clifford Codes](https://arxiv.org/pdf/quant-ph/0010076v2)** | 2023-11-27 | <details><summary>Show</summary><p>Knill introduced a generalization of stabilizer codes, in this note called Clifford codes. It remained unclear whether or not Clifford codes can be superior to stabilizer codes. We show that Clifford codes are stabilizer codes provided that the abstract error group has an abelian index group. In particular, if the errors are modelled by tensor products of Pauli matrices, then the associated Clifford codes are necessarily stabilizer codes.</p></details> | <details><summary>9 pag...</summary><p>9 pages, LaTeX2e. Minor changes. Title changed by request of IEEE Trans. IT</p></details> |
| **[Quaternary Conjucyclic Codes with an Application to EAQEC Codes](https://arxiv.org/pdf/2309.01983v1)** | 2023-09-06 | <details><summary>Show</summary><p>Conjucyclic codes are part of a family of codes that includes cyclic, constacyclic, and quasi-cyclic codes, among others. Despite their importance in quantum error correction, they have not received much attention in the literature. This paper focuses on additive conjucyclic (ACC) codes over $\mathbb{F}_4$ and investigates their properties. Specifically, we derive the duals of ACC codes using a trace inner product and obtain the trace hull and its dimension. Also, establish a necessary and sufficient condition for an additive code to have a complementary dual (ACD). Additionally, we identify a necessary condition for an additive conjucyclic complementary pair of codes over $\mathbb{F}_4$. Furthermore, we show that the trace code of an ACC code is cyclic and provide a condition for the trace code of an ACC code to be LCD. To demonstrate the practical application of our findings, we construct some good entanglement-assisted quantum error-correcting (EAQEC) codes using the trace code of ACC codes.</p></details> |  |
| **[Commenting Higher-level Code Unit: Full Code, Reduced Code, or Hierarchical Code Summarization](https://arxiv.org/pdf/2503.10737v1)** | 2025-03-17 | <details><summary>Show</summary><p>Commenting code is a crucial activity in software development, as it aids in facilitating future maintenance and updates. To enhance the efficiency of writing comments and reduce developers' workload, researchers has proposed various automated code summarization (ACS) techniques to automatically generate comments/summaries for given code units. However, these ACS techniques primarily focus on generating summaries for code units at the method level. There is a significant lack of research on summarizing higher-level code units, such as file-level and module-level code units, despite the fact that summaries of these higher-level code units are highly useful for quickly gaining a macro-level understanding of software components and architecture. To fill this gap, in this paper, we conduct a systematic study on how to use LLMs for commenting higher-level code units, including file level and module level. These higher-level units are significantly larger than method-level ones, which poses challenges in handling long code inputs within LLM constraints and maintaining efficiency. To address these issues, we explore various summarization strategies for ACS of higher-level code units, which can be divided into three types: full code summarization, reduced code summarization, and hierarchical code summarization. The experimental results suggest that for summarizing file-level code units, using the full code is the most effective approach, with reduced code serving as a cost-efficient alternative. However, for summarizing module-level code units, hierarchical code summarization becomes the most promising strategy. In addition, inspired by the research on method-level ACS, we also investigate using the LLM as an evaluator to evaluate the quality of summaries of higher-level code units. The experimental results demonstrate that the LLM's evaluation results strongly correlate with human evaluations.</p></details> |  |
| **[CRC Codes as Error Correction Codes](https://arxiv.org/pdf/2104.13663v1)** | 2024-10-28 | <details><summary>Show</summary><p>CRC codes have long since been adopted in a vast range of applications. The established notion that they are suitable primarily for error detection can be set aside through use of the recently proposed Guessing Random Additive Noise Decoding (GRAND). Hard-detection (GRAND-SOS) and soft-detection (ORBGRAND) variants can decode any short, high-rate block code, making them suitable for error correction of CRC-coded data. When decoded with GRAND, short CRC codes have error correction capability that is at least as good as popular codes such as BCH codes, but with no restriction on either code length or rate. The state-of-the-art CA-Polar codes are concatenated CRC and Polar codes. For error correction, we find that the CRC is a better short code than either Polar or CA-Polar codes. Moreover, the standard CA-SCL decoder only uses the CRC for error detection and therefore suffers severe performance degradation in short, high rate settings when compared with the performance GRAND provides, which uses all of the CA-Polar bits for error correction. Using GRAND, existing systems can be upgraded from error detection to low-latency error correction without re-engineering the encoder, and additional applications of CRCs can be found in IoT, Ultra-Reliable Low Latency Communication (URLLC), and beyond. The universality of GRAND, its ready parallelized implementation in hardware, and the good performance of CRC as codes make their combination a viable solution for low-latency applications.</p></details> | <details><summary>This ...</summary><p>This work has been submitted to the IEEE for possible publication</p></details> |
| **[Practical Product Code Construction of Polar Codes](https://arxiv.org/pdf/1910.06803v1)** | 2020-04-22 | <details><summary>Show</summary><p>In this paper, we study the connection between polar codes and product codes. Our analysis shows that the product of two polar codes is again a polar code, and we provide guidelines to compute its frozen set on the basis of the frozen sets of the component polar codes. Moreover, we show how polar codes can be described as irregular product codes. We propose a two-step decoder for long polar codes taking advantage of this dual nature to heavily reduce decoding latency. Finally, we show that the proposed decoding technique outperforms both standard polar codes and state-of-the-art codes for optical communications under latency constraints.</p></details> | <details><summary>Under...</summary><p>Under review in IEEE Transactions on Signal Processing. arXiv admin note: text overlap with arXiv:1901.06892</p></details> |
| **[Convolutional-Code-Specific CRC Code Design](https://arxiv.org/pdf/1506.02990v1)** | 2015-06-10 | <details><summary>Show</summary><p>Cyclic redundancy check (CRC) codes check if a codeword is correctly received. This paper presents an algorithm to design CRC codes that are optimized for the code-specific error behavior of a specified feedforward convolutional code. The algorithm utilizes two distinct approaches to computing undetected error probability of a CRC code used with a specific convolutional code. The first approach enumerates the error patterns of the convolutional code and tests if each of them is detectable. The second approach reduces complexity significantly by exploiting the equivalence of the undetected error probability to the frame error rate of an equivalent catastrophic convolutional code. The error events of the equivalent convolutional code are exactly the undetectable errors for the original concatenation of CRC and convolutional codes. This simplifies the computation because error patterns do not need to be individually checked for detectability. As an example, we optimize CRC codes for a commonly used 64-state convolutional code for information length k=1024 demonstrating significant reduction in undetected error probability compared to the existing CRC codes with the same degrees. For a fixed target undetected error probability, the optimized CRC codes typically require 2 fewer bits.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 8 figures, journal paper</p></details> |
| **[On The Effect Of Code Review On Code Smells](https://arxiv.org/pdf/1912.10098v1)** | 2019-12-24 | <details><summary>Show</summary><p>Code smells are symptoms of poor design quality. Since code review is a process that also aims at improving code quality, we investigate whether and how code review influences the severity of code smells. In this study, we analyze more than 21,000 code reviews belonging to seven Java open-source projects; we find that active and participated code reviews have a significant influence on the likelihood of reducing the severity of code smells. This result seems to confirm the expectations around code review's influence on code quality. However, by manually investigating 365 cases in which the severity of a code smell in a file was reduced with a review, we found that-in 95% of the cases-the reduction was a side effect of changes that reviewers requested on matters unrelated to code smells. Data and materials [https://doi.org/10.5281/zenodo.3588501].</p></details> | <details><summary>Publi...</summary><p>Published in the proceedings of the 27th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER 2020)</p></details> |
| **[Coding Concepts and Reed-Solomon Codes](https://arxiv.org/pdf/2205.01044v1)** | 2022-05-03 | <details><summary>Show</summary><p>The material in this book is presented to graduate students in Information and Communication theory. The idea is that we give an introduction to particular applications of information theory and coding in digital communications. The goal is to bring understanding of the underlying concepts, both in theory as well as in practice. We mainly concentrate on our own research results. After showing obtainable performance, we give a specific implementation using Reed-Solomon (RS) codes. The reason for using RS codes is that they can be seen as optimal codes with maximum obtainable minimum distance. Furthermore, the structure of RS codes enables specific applications that fit perfectly into the developed concepts. We do not intend to develop the theory of error correcting codes.</p></details> |  |
| **[Algebraic codes for Slepian-Wolf code design](https://arxiv.org/pdf/1106.2792v1)** | 2011-06-15 | <details><summary>Show</summary><p>Practical constructions of lossless distributed source codes (for the Slepian-Wolf problem) have been the subject of much investigation in the past decade. In particular, near-capacity achieving code designs based on LDPC codes have been presented for the case of two binary sources, with a binary-symmetric correlation. However, constructing practical codes for the case of non-binary sources with arbitrary correlation remains by and large open. From a practical perspective it is also interesting to consider coding schemes whose performance remains robust to uncertainties in the joint distribution of the sources. In this work we propose the usage of Reed-Solomon (RS) codes for the asymmetric version of this problem. We show that algebraic soft-decision decoding of RS codes can be used effectively under certain correlation structures. In addition, RS codes offer natural rate adaptivity and performance that remains constant across a family of correlation structures with the same conditional entropy. The performance of RS codes is compared with dedicated and rate adaptive multistage LDPC codes (Varodayan et al. '06), where each LDPC code is used to compress the individual bit planes. Our simulations show that in classical Slepian-Wolf scenario, RS codes outperform both dedicated and rate-adaptive LDPC codes under $q$-ary symmetric correlation, and are better than rate-adaptive LDPC codes in the case of sparse correlation models, where the conditional distribution of the sources has only a few dominant entries. In a feedback scenario, the performance of RS codes is comparable with both designs of LDPC codes. Our simulations also demonstrate that the performance of RS codes in the presence of inaccuracies in the joint distribution of the sources is much better as compared to multistage LDPC codes.</p></details> | <details><summary>5 pag...</summary><p>5 pages, accepted by ISIT 2011</p></details> |
| **[AI Coding: Learning to Construct Error Correction Codes](https://arxiv.org/pdf/1901.05719v2)** | 2019-10-31 | <details><summary>Show</summary><p>In this paper, we investigate an artificial-intelligence (AI) driven approach to design error correction codes (ECC). Classic error correction code was designed upon coding theory that typically defines code properties (e.g., hamming distance, subchannel reliability, etc.) to reflect code performance. Its code design is to optimize code properties. However, an AI-driven approach doesn't necessarily rely on coding theory any longer. Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by AI algorithms and the code evaluator provides code performance metric measurements. The code constructor keeps improving the code construction to maximize code performance that is evaluated by the code evaluator. As examples, we construct linear block codes and polar codes with reinforcement learning (RL) and evolutionary algorithms. The results show that comparable code performance can be achieved with respect to the existing codes. It is noteworthy that our method can provide superior performances where existing classic constructions fail to achieve optimum for a specific decoder (e.g., list decoding for polar codes).</p></details> | <details><summary>14 pa...</summary><p>14 pages; 15 figures; Accepted for publication in the IEEE Transactions on Communications</p></details> |
| **[Self-orthogonal codes from plateaued functions and their applications in quantum codes and LCD codes](https://arxiv.org/pdf/2502.11599v2)** | 2025-08-06 | <details><summary>Show</summary><p>Self-orthogonal codes have received great attention due to their important applications in quantum codes, LCD codes and lattices. Recently, several families of self-orthogonal codes containing the all-$1$ vector were constructed by augmentation technique. In this paper, utilizing plateaued functions, we construct some classes of linear codes which do not contain the all-$1$ vector. We also investigate their punctured codes. The weight distributions of the constructed codes are explicitly determined. Under certain conditions, these codes are proved to be self-orthogonal. Furthermore, some classes of optimal linear codes are obtained from their duals. Using the self-orthogonal punctured codes, we also construct several new families of at least almost optimal quantum codes and optimal LCD codes.</p></details> |  |
| **[Polar Codes' Simplicity, Random Codes' Durability](https://arxiv.org/pdf/1912.08995v1)** | 2020-12-14 | <details><summary>Show</summary><p>Over any discrete memoryless channel, we build codes such that: for one, their block error probabilities and code rates scale like random codes'; and for two, their encoding and decoding complexities scale like polar codes'. Quantitatively, for any constants $π,ρ>0$ such that $π+2ρ<1$, we construct a sequence of error correction codes with block length $N$ approaching infinity, block error probability $\exp(-N^π)$, code rate $N^{-ρ}$ less than the Shannon capacity, and encoding and decoding complexity $O(N\log N)$ per code block. The putative codes take uniform $ς$-ary messages for sender's choice of prime $ς$. The putative codes are optimal in the following manner: Should $π+2ρ>1$, no such codes exist for generic channels regardless of alphabet and complexity.</p></details> | <details><summary>55 pa...</summary><p>55 pages, 8 figures, 2 tables</p></details> |
| **[Code Search: A Survey of Techniques for Finding Code](https://arxiv.org/pdf/2204.02765v3)** | 2022-10-06 | <details><summary>Show</summary><p>The immense amounts of source code provide ample challenges and opportunities during software development. To handle the size of code bases, developers commonly search for code, e.g., when trying to find where a particular feature is implemented or when looking for code examples to reuse. To support developers in finding relevant code, various code search engines have been proposed. This article surveys 30 years of research on code search, giving a comprehensive overview of challenges and techniques that address them. We discuss the kinds of queries that code search engines support, how to preprocess and expand queries, different techniques for indexing and retrieving code, and ways to rank and prune search results. Moreover, we describe empirical studies of code search in practice. Based on the discussion of prior work, we conclude the article with an outline of challenges and opportunities to be addressed in the future.</p></details> |  |
| **[Cyclic LRC Codes, binary LRC codes, and upper bounds on the distance of cyclic codes](https://arxiv.org/pdf/1603.08878v1)** | 2017-02-10 | <details><summary>Show</summary><p>We consider linear cyclic codes with the locality property, or locally recoverable codes (LRC codes). A family of LRC codes that generalize the classical construction of Reed-Solomon codes was constructed in a recent paper by I. Tamo and A. Barg (IEEE Trans. Inform. Theory, no. 8, 2014). In this paper we focus on optimal cyclic codes that arise from this construction. We give a characterization of these codes in terms of their zeros, and observe that there are many equivalent ways of constructing optimal cyclic LRC codes over a given field. We also study subfield subcodes of cyclic LRC codes (BCH-like LRC codes) and establish several results about their locality and minimum distance. The locality parameter of a cyclic code is related to the dual distance of this code, and we phrase our results in terms of upper bounds on the dual distance.</p></details> | <details><summary>12pp....</summary><p>12pp., submitted for publication. An extended abstract of this submission was posted earlier as arXiv:1502.01414 and was published in Proceedings of the 2015 IEEE International Symposium on Information Theory, Hong Kong, China, June 14-19, 2015, pp. 1262--1266</p></details> |
| **[Punctured Binary Simplex Codes as LDPC codes](https://arxiv.org/pdf/2210.03537v1)** | 2022-10-10 | <details><summary>Show</summary><p>Digital data transfer can be protected by means of suitable error correcting codes. Among the families of state-of-the-art codes, LDPC (Low Density Parity-Check) codes have received a great deal of attention recently, because of their performance and flexibility of operation, in wireless and mobile radio channels, as well as in cable transmission systems. In this paper, we present a class of rate-adaptive LDPC codes, obtained as properly punctured simplex codes. These codes allow for the use of an efficient soft-decision decoding algorithm, provided that a condition called row-column constraint is satisfied. This condition is tested on small-length codes, and then extended to medium-length codes. The puncturing operations we apply do not influence the satisfaction of the row-column constraint, assuring that a wide range of code rates can be obtained. We can reach code rates remarkably higher than those obtainable by the original simplex code, and the price in terms of minimum distance turns out to be relatively small, leading to interesting trade-offs in the resulting asymptotic coding gain.</p></details> |  |
| **[A Code Equivalence between Secure Network and Index Coding](https://arxiv.org/pdf/1804.09888v1)** | 2018-04-27 | <details><summary>Show</summary><p>A code equivalence between index coding and network coding was established, which shows that any index-coding instance can be mapped to a network-coding instance, for which any index code can be translated to a network code with the same decoding-error performance, and vice versa. Also, any network-coding instance can be mapped to an index-coding instance with a similar code translation. In this paper, we extend the equivalence to secure index coding and secure network coding, where eavesdroppers are present in the networks, and any code construction needs to guarantee security constraints in addition to decoding-error performance.</p></details> | <details><summary>The j...</summary><p>The journal version of conference papers "An Equivalence Between Secure Network and Index Coding" (Globecom-NetCod 2016) and "Secure Network-Index Code Equivalence: Extension to Non-zero Error and Leakage" (ISIT 2018)</p></details> |
| **[On Quasi-Cyclic Codes as a Generalization of Cyclic Codes](https://arxiv.org/pdf/1108.3754v2)** | 2012-05-30 | <details><summary>Show</summary><p>In this article we see quasi-cyclic codes as block cyclic codes. We generalize some properties of cyclic codes to quasi-cyclic ones such as generator polynomials and ideals. Indeed we show a one-to-one correspondence between l-quasi-cyclic codes of length m and ideals of M_l(Fq)[X]/(X^m-1). This permits to construct new classes of codes, namely quasi-BCH and quasi-evaluation codes. We study the parameters of such codes and propose a decoding algorithm up to half the designed minimum distance. We even found one new quasi-cyclic code with better parameters than known [189, 11, 125]_F4 and 48 derivated codes beating the known bounds as well.</p></details> | (18/08/2011) |
| **[Toric Varieties and Codes, Error-correcting Codes, Quantum Codes, Secret Sharing and Decoding](https://arxiv.org/pdf/1808.06487v2)** | 2019-12-03 | <details><summary>Show</summary><p>Toric varieties and their associated toric codes, as well as determination of their parameters with intersection theory, are presented in the two dimensional case. Linear Secret Sharing Schemes with strong multiplication are constructed from toric varieties and codes by the J. L. Massey construction. Asymmetric Quantum Codes are obtained from toric codes by the A.R. Calderbank, P.W. Shor and A.M. Steane construction of stabilizer codes from linear codes containing their dual codes. Decoding of a class of toric codes is presented.</p></details> | 24 pages, 6 figures |
| **[On Decoding of Generalized Concatenated Codes and Matrix-Product Codes](https://arxiv.org/pdf/2004.03538v1)** | 2020-04-08 | <details><summary>Show</summary><p>Generalized concatenated codes were introduced in the 1970s by Zinoviev. There are many types of codes in the literature that are known by other names that can be viewed as generalized concatenated codes. Examples include matrix-product codes, multilevel codes and generalized cascade codes. Decoding algorithms for generalized concatenated codes were developed during the 1970s and 1980s. However, their use does not appear to be as widespread as it should, especially for codes that are known by other names but can be viewed as generalized concatenated codes. In this paper we review the decoding algorithms for concatenated codes, generalized concatenated codes and matrix-product codes, and clarify the connection between matrix-product codes and generalized concatenated codes. We present a small improvement to the decoding algorithm for concatenated codes. We also extend the decoding algorithms from errors-only decoders to error-and-erasure decoders. Furthermore, we improve the upper bound on the computational complexity of the decoding algorithm in the case of matrix-product codes where the generator matrix for the inner code is non-singular by columns.</p></details> |  |
| **[A survey on product codes and 2-D codes](https://arxiv.org/pdf/2112.14206v2)** | 2022-01-03 | <details><summary>Show</summary><p>One of the simplest way of combining codes to form new codes is to take their direct product. Direct product of cyclic codes and various generalizations have been studied for many years. In this note, we survey cyclic product codes, direct product of various generalizations of cyclic codes and their properties.</p></details> | <details><summary>40 pa...</summary><p>40 pages, typos corrected, references added. arXiv admin note: text overlap with arXiv:1512.06690, arXiv:1505.02238, arXiv:1301.6231 by other authors</p></details> |
| **[A family of codes with locality containing optimal codes](https://arxiv.org/pdf/2101.07629v1)** | 2021-01-20 | <details><summary>Show</summary><p>Locally recoverable codes were introduced by Gopalan et al. in 2012, and in the same year Prakash et al. introduced the concept of codes with locality, which are a type of locally recoverable codes. In this work we introduce a new family of codes with locality, which are subcodes of a certain family of evaluation codes. We determine the dimension of these codes, and also bounds for the minimum distance. We present the true values of the minimum distance in special cases, and also show that elements of this family are "optimal codes", as defined by Prakash et al.</p></details> |  |
| **[Coding for Network Coding](https://arxiv.org/pdf/0711.3935v1)** | 2007-11-27 | <details><summary>Show</summary><p>We consider communication over a noisy network under randomized linear network coding. Possible error mechanism include node- or link- failures, Byzantine behavior of nodes, or an over-estimate of the network min-cut. Building on the work of Koetter and Kschischang, we introduce a probabilistic model for errors. We compute the capacity of this channel and we define an error-correction scheme based on random sparse graphs and a low-complexity decoding algorithm. By optimizing over the code degree profile, we show that this construction achieves the channel capacity in complexity which is jointly quadratic in the number of coded information bits and sublogarithmic in the error probability.</p></details> | <details><summary>12 pa...</summary><p>12 pages, 2 ps figures</p></details> |
| **[New Quasi-Cyclic Codes from Simplex Codes](https://arxiv.org/pdf/cs/0609006v2)** | 2007-07-16 | <details><summary>Show</summary><p>As a generalization of cyclic codes, quasi-cyclic (QC) codes contain many good linear codes. But quasi-cyclic codes studied so far are mainly limited to one generator (1-generator) QC codes. In this correspondence, 2-generator and 3-generator QC codes are studied, and many good, new QC codes are constructed from simplex codes. Some new binary QC codes or related codes, that improve the bounds on maximum minimum distance for binary linear codes are constructed. They are 5-generator QC [93, 17, 34] and [254, 23, 102] codes, and related [96, 17, 36], [256, 23, 104] codes.</p></details> | 3 pages |
| **[Multishot Codes for Network Coding using Rank-Metric Codes](https://arxiv.org/pdf/1001.2059v2)** | 2010-04-13 | <details><summary>Show</summary><p>The multiplicative-additive finite-field matrix channel arises as an adequate model for linear network coding systems when links are subject to errors and erasures, and both the network topology and the network code are unknown. In a previous work we proposed a general construction of multishot codes for this channel based on the multilevel coding theory. Herein we apply this construction to the rank-metric space, obtaining multishot rank-metric codes which, by lifting, can be converted to codes for the aforementioned channel. We also adapt well-known encoding and decoding algorithms to the considered situation.</p></details> | <details><summary>6 pag...</summary><p>6 pages, 4 figures. Replaced the extended injection distance (and doubtful unproven statements about it) with the extended subspace distance; some other minor corrections and points clarified.</p></details> |
| **[Code Attestation with Compressed Instruction Code](https://arxiv.org/pdf/1102.5322v1)** | 2011-02-28 | <details><summary>Show</summary><p>Available purely software based code attestation protocols have recently been shown to be cheatable. In this work we propose to upload compressed instruction code to make the code attestation protocol robust against a so called compresssion attack. The described secure code attestation protocol makes use of recently proposed microcontroller architectures for reading out compressed instruction code. We point out that the proposed concept only makes sense if the provided cost/benefit ratio for the aforementioned microcontroller is higher than an alternative hardware based solution requiring a tamperresistant hardware module.</p></details> | 7 pages |
| **[Non-Binary Polar Codes using Reed-Solomon Codes and Algebraic Geometry Codes](https://arxiv.org/pdf/1007.3661v1)** | 2010-07-22 | <details><summary>Show</summary><p>Polar codes, introduced by Arikan, achieve symmetric capacity of any discrete memoryless channels under low encoding and decoding complexity. Recently, non-binary polar codes have been investigated. In this paper, we calculate error probability of non-binary polar codes constructed on the basis of Reed-Solomon matrices by numerical simulations. It is confirmed that 4-ary polar codes have significantly better performance than binary polar codes on binary-input AWGN channel. We also discuss an interpretation of polar codes in terms of algebraic geometry codes, and further show that polar codes using Hermitian codes have asymptotically good performance.</p></details> | <details><summary>5 pag...</summary><p>5 pages, 3 figures, to appear in ITW 2010 Dublin</p></details> |
| **[LOCO Codes: Lexicographically-Ordered Constrained Codes](https://arxiv.org/pdf/1902.10898v5)** | 2020-05-26 | <details><summary>Show</summary><p>Line codes make it possible to mitigate interference, to prevent short pulses, and to generate streams of bipolar signals with no direct-current (DC) power content through balancing. They find application in magnetic recording (MR) devices, in Flash devices, in optical recording devices, and in some computer standards. This paper introduces a new family of fixed-length, binary constrained codes, named lexicographically-ordered constrained codes (LOCO codes), for bipolar non-return-to-zero signaling. LOCO codes are capacity-achieving, the lexicographic indexing enables simple, practical encoding and decoding, and this simplicity is demonstrated through analysis of circuit complexity. LOCO codes are easy to balance, and their inherent symmetry minimizes the rate loss with respect to unbalanced codes having the same constraints. Furthermore, LOCO codes that forbid certain patterns can be used to alleviate inter-symbol interference in MR systems and inter-cell interference in Flash systems. Numerical results demonstrate a gain of up to 10% in rate achieved by LOCO codes with respect to other practical constrained codes, including run-length-limited codes, designed for the same purpose. Simulation results suggest that it is possible to achieve a channel density gain of about 20% in MR systems by using a LOCO code to encode only the parity bits, limiting the rate loss, of a low-density parity-check code before writing.</p></details> | <details><summary>17 pa...</summary><p>17 pages (double column), 2 figures, accepted at the IEEE Transactions on Information Theory (TIT), the short version was accepted at the IEEE Information Theory Workshop (ITW), this version reflects comments from reviewers at TIT and ITW</p></details> |
| **[Index Coding with Coded Side-Information](https://arxiv.org/pdf/1501.00077v1)** | 2016-11-18 | <details><summary>Show</summary><p>This letter investigates a new class of index coding problems. One sender broadcasts packets to multiple users, each desiring a subset, by exploiting prior knowledge of linear combinations of packets. We refer to this class of problems as index coding with coded side-information. Our aim is to characterize the minimum index code length that the sender needs to transmit to simultaneously satisfy all user requests. We show that the optimal binary vector index code length is equal to the minimum rank (minrank) of a matrix whose elements consist of the sets of desired packet indices and side- information encoding matrices. This is the natural extension of matrix minrank in the presence of coded side information. Using the derived expression, we propose a greedy randomized algorithm to minimize the rank of the derived matrix.</p></details> | <details><summary>A sho...</summary><p>A short version will be appeared in IEEE Communications Letter</p></details> |
| **[Fuzzy linear codes based on nested linear codes](https://arxiv.org/pdf/2306.06903v2)** | 2024-09-10 | <details><summary>Show</summary><p>In this paper, we describe a correspondence between a fuzzy linear code and a family of nested linear codes. We also describe the arithmetic of fuzzy linear codes. As a special class of nested linear codes, we consider a family of nested self-orthogonal codes. A linear code is self-orthogonal if it is contained in its dual and self-dual if it is equal to its dual. We introduce a definition of fuzzy self-dual or self-orthogonal codes which include classical self-dual or self-orthogonal codes. As examples, we construct several interesting classes of fuzzy linear codes including fuzzy Hamming codes, fuzzy Golay codes, and fuzzy Reed-Muller codes. We also give a general decoding algorithm for fuzzy linear codes.</p></details> |  |
| **[Zipper Codes](https://arxiv.org/pdf/2203.10120v2)** | 2024-10-31 | <details><summary>Show</summary><p>Zipper codes are a framework for describing spatially-coupled product-like codes. Many well-known codes, such as staircase codes and braided block codes, are subsumed into this framework. New types of codes such as tiled diagonal and delayed diagonal zipper codes are introduced along with their software simulation results. Stall patterns that can arise in iterative decoding are analyzed, giving a means of error floor estimation.</p></details> | <details><summary>Accep...</summary><p>Accepted for publication on JLT, updated reference for oFEC</p></details> |
| **[Submodule codes as spherical codes in buildings](https://arxiv.org/pdf/2202.13370v3)** | 2023-09-25 | <details><summary>Show</summary><p>We give a generalization of subspace codes by means of codes of modules over finite commutative chain rings. We define a new class of Sperner codes and use results from extremal combinatorics to prove the optimality of such codes in different cases. Moreover, we explain the connection with Bruhat-Tits buildings and show how our codes are the buildings' analogue of spherical codes in the Euclidean sense.</p></details> | <details><summary>21 pa...</summary><p>21 pages, revision including the referees' suggestions, to appear in Designs, Codes and Cryptography</p></details> |
| **[Column Twisted Reed-Solomon Codes as MDS Codes](https://arxiv.org/pdf/2507.08755v1)** | 2025-07-14 | <details><summary>Show</summary><p>In this paper, we study column twisted Reed-Solomon(TRS) codes. We establish some conditions for column TRS codes to be MDS codes and show that the dimension of their Schur square codes is $2k$. Consequently, these TRS codes are not equivalent to Reed-Solomon(RS) codes. Moreover, this construction method provides more flexible parameters compared to previous twisted generalized Reed-Solomon(TGRS) code constructions. For large odd prime power $q$, different from the systematically constructed TGRS codes whose length was previously limited to $\frac{q+1}{2}$, our construction achieves code lengths up to $\frac{q+3}{2}$. Finally, we present the dual codes of column TRS codes. This paper provides a new approach to construct MDS codes by adding column vectors to generator matrix of RS codes.</p></details> |  |
| **[Fuchsian codes with arbitrarily high code rate](https://arxiv.org/pdf/1410.6094v1)** | 2014-10-23 | <details><summary>Show</summary><p>Recently, so-called Fuchsian codes have been proposed in [I. Blanco-Chacón et al., "Nonuniform Fuchsian codes for noisy channels", J. of the Franklin Institute 2014] for communication over channels subject to additive white Gaussian noise (AWGN). The two main advantages of Fuchsian codes are their ability to compress information, i.e., high code rate, and their logarithmic decoding complexity. In this paper, we improve the first property further by constructing Fuchsian codes with arbitrarily high code rates while maintaining logarithmic decoding complexity. Namely, in the case of Fuchsian groups derived from quaternion algebras over totally real fields we obtain a code rate that is proportional to the degree of the base field. In particular, we consider arithmetic Fuchsian groups of signature (1;e) to construct explicit codes having code rate six, meaning that we can transmit six independent integers during one channel use.</p></details> |  |
| **[Code Llama: Open Foundation Models for Code](https://arxiv.org/pdf/2308.12950v3)** | 2024-02-02 | <details><summary>Show</summary><p>We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.</p></details> |  |
| **[Robust Locally Testable Codes and Products of Codes](https://arxiv.org/pdf/cs/0408066v1)** | 2007-07-16 | <details><summary>Show</summary><p>We continue the investigation of locally testable codes, i.e., error-correcting codes for whom membership of a given word in the code can be tested probabilistically by examining it in very few locations. We give two general results on local testability: First, motivated by the recently proposed notion of {\em robust} probabilistically checkable proofs, we introduce the notion of {\em robust} local testability of codes. We relate this notion to a product of codes introduced by Tanner, and show a very simple composition lemma for this notion. Next, we show that codes built by tensor products can be tested robustly and somewhat locally, by applying a variant of a test and proof technique introduced by Raz and Safra in the context of testing low-degree multivariate polynomials (which are a special case of tensor codes). Combining these two results gives us a generic construction of codes of inverse polynomial rate, that are testable with poly-logarithmically many queries. We note these locally testable tensor codes can be obtained from {\em any} linear error correcting code with good distance. Previous results on local testability, albeit much stronger quantitatively, rely heavily on algebraic properties of the underlying codes.</p></details> |  |
| **[ELF Codes: Concatenated Codes with an Expurgating Linear Function as the Outer Code](https://arxiv.org/pdf/2306.07467v2)** | 2023-08-02 | <details><summary>Show</summary><p>An expurgating linear function (ELF) is a linear outer code that disallows the low-weight codewords of the inner code. ELFs can be designed either to maximize the minimum distance or to minimize the codeword error rate (CER) of the expurgated code. A list-decoding sieve of the inner code starting from the noiseless all-zeros codeword is an efficient way to identify ELFs that maximize the minimum distance of the expurgated code. For convolutional inner codes, this paper provides distance spectrum union (DSU) upper bounds on the CER of the concatenated code. For short codeword lengths, ELFs transform a good inner code into a great concatenated code. For a constant message size of $K=64$ bits or constant codeword blocklength of $N=152$ bits, an ELF can reduce the gap at CER $10^{-6}$ between the DSU and the random-coding union (RCU) bounds from over 1 dB for the inner code alone to 0.23 dB for the concatenated code. The DSU bounds can also characterize puncturing that mitigates the rate overhead of the ELF while maintaining the DSU-to-RCU gap. The reduction in DSU-to-RCU gap comes with a minimal increase in average complexity at desired CER operating points. List Viterbi decoding guided by the ELF approaches maximum likelihood (ML) decoding of the concatenated code, and average list size converges to 1 as SNR increases. Thus, average complexity is similar to Viterbi decoding on the trellis of the inner code at high SNR. For rare large-magnitude noise events, which occur less often than the FER of the inner code, a deep search in the list finds the ML codeword.</p></details> | <details><summary>6 arX...</summary><p>6 arXiv pages (actual ISTC paper is 5 pages with more compressed spacing), 6 figures, accepted to the 2023 International Symposium on Techniques in Coding. Latest version is Camera-Ready version for ISTC edited for clarity and to reflect reviewer suggestions and references were added</p></details> |
| **[The extended codes of a family of reversible MDS cyclic codes](https://arxiv.org/pdf/2307.04076v1)** | 2023-07-11 | <details><summary>Show</summary><p>A linear code with parameters $[n, k, n-k+1]$ is called a maximum distance separable (MDS for short) code. A linear code with parameters $[n, k, n-k]$ is said to be almost maximum distance separable (AMDS for short). A linear code is said to be near maximum distance separable (NMDS for short) if both the code and its dual are AMDS. MDS codes are very important in both theory and practice. There is a classical construction of a $[q+1, 2u-1, q-2u+3]$ MDS code for each $u$ with $1 \leq u \leq \lfloor\frac{q+1}2\rfloor$, which is a reversible and cyclic code. The objective of this paper is to study the extended codes of this family of MDS codes. Two families of MDS codes and several families of NMDS codes are obtained. The NMDS codes have applications in finite geometry, cryptography and distributed and cloud data storage systems. The weight distributions of some of the extended codes are determined.</p></details> |  |
| **[Steganographic Codes -- a New Problem of Coding Theory](https://arxiv.org/pdf/cs/0505072v1)** | 2009-09-29 | <details><summary>Show</summary><p>To study how to design steganographic algorithm more efficiently, a new coding problem -- steganographic codes (abbreviated stego-codes) -- is presented in this paper. The stego-codes are defined over the field with $q(q\ge2)$ elements. Firstly a method of constructing linear stego-codes is proposed by using the direct sum of vector subspaces. And then the problem of linear stego-codes is converted to an algebraic problem by introducing the concept of $t$th dimension of vector space. And some bounds on the length of stego-codes are obtained, from which the maximum length embeddable (MLE) code is brought up. It is shown that there is a corresponding relation between MLE codes and perfect error-correcting codes. Furthermore the classification of all MLE codes and a lower bound on the number of binary MLE codes are obtained based on the corresponding results on perfect codes. Finally hiding redundancy is defined to value the performance of stego-codes.</p></details> | <details><summary>7 pag...</summary><p>7 pages with 1 figure</p></details> |
| **[Generalized Adaptive Network Coded Cooperation (GANCC): A Unified Framework for Network Coding and Channel Coding](https://arxiv.org/pdf/1002.3629v1)** | 2010-02-22 | <details><summary>Show</summary><p>This paper considers distributed coding for multi-source single-sink data collection wireless networks. A unified framework for network coding and channel coding, termed "generalized adaptive network coded cooperation" (GANCC), is proposed. Key ingredients of GANCC include: matching code graphs with the dynamic network graphs on-the-fly, and integrating channel coding with network coding through circulant low-density parity-check codes. Several code constructing methods and several families of sparse-graph codes are proposed, and information theoretical analysis is performed. It is shown that GANCC is simple to operate, adaptive in real time, distributed in nature, and capable of providing remarkable coding gains even with a very limited number of cooperating users.</p></details> |  |
| **[A family of linear codes that are either non-GRS MDS codes or NMDS codes](https://arxiv.org/pdf/2401.04360v3)** | 2025-01-28 | <details><summary>Show</summary><p>Both maximum distance separable (MDS) codes that are not equivalent to generalized Reed-Solomon (GRS) codes (non-GRS MDS codes) and near MDS (NMDS) codes have nice applications in communication and storage systems. In this paper, we introduce and study a new family of linear codes involving their parameters, weight distributions, and self-orthogonal properties. We prove that such codes are either non-GRS MDS codes or NMDS codes, and hence, they can produce as many of the desired codes as possible. We also completely determine their weight distributions with the help of the solutions to some subset sum problems. A sufficient and necessary condition for such codes to be self-orthogonal is characterized. Based on this condition, we further deduce that there are no self-dual codes in this class of linear codes and explicitly construct two new classes of almost self-dual codes.</p></details> | <details><summary>The n...</summary><p>The new revised version for submitted</p></details> |
| **[Decoding color codes by projection onto surface codes](https://arxiv.org/pdf/1308.6207v1)** | 2014-01-22 | <details><summary>Show</summary><p>We propose a new strategy to decode color codes, which is based on the projection of the error onto three surface codes. This provides a method to transform every decoding algorithm of surface codes into a decoding algorithm of color codes. Applying this idea to a family of hexagonal color codes, with the perfect matching decoding algorithm for the three corresponding surface codes, we find a phase error threshold of approximately 8.7%. Finally, our approach enables us to establish a general lower bound on the error threshold of a family of color codes depending on the threshold of the three corresponding surface codes. These results are based on a chain complex interpretation of surface codes and color codes.</p></details> | 24 pages |
| **[Spread Codes and Spread Decoding in Network Coding](https://arxiv.org/pdf/0805.0507v2)** | 2016-11-17 | <details><summary>Show</summary><p>In this paper we introduce the class of Spread Codes for the use in random network coding. Spread Codes are based on the construction of spreads in finite projective geometry. The major contribution of the paper is an efficient decoding algorithm of spread codes up to half the minimum distance.</p></details> |  |
| **[Generalized Fractional Repetition Codes for Binary Coded Computations](https://arxiv.org/pdf/2109.10484v3)** | 2025-01-14 | <details><summary>Show</summary><p>This paper addresses the gradient coding and coded matrix multiplication problems in distributed optimization and coded computing. We present a numerically stable binary coding method which overcomes the drawbacks of the \textit{Fractional Repetition Coding} gradient coding method proposed by Tandon et al., and can also be leveraged by coded computing networks whose servers are of heterogeneous nature. Specifically, we propose a construction for fractional repetition gradient coding; while ensuring that the generator matrix remains close to perfectly balanced for any set of coded parameters, as well as a low complexity decoding step. The proposed binary encoding avoids operations over the real and complex numbers which are inherently numerically unstable, thereby enabling numerically stable distributed encodings of the partial gradients. We then make connections between gradient coding and coded matrix multiplication. Specifically, we show that any gradient coding scheme can be extended to coded matrix multiplication. Furthermore, we show how the proposed binary gradient coding scheme can be used to construct two different coded matrix multiplication schemes, each achieving different trade-offs.</p></details> | 27 pages, 2 tables |
| **[Quaternary linear codes and related binary subfield codes](https://arxiv.org/pdf/2112.15462v1)** | 2022-01-03 | <details><summary>Show</summary><p>In this paper, we mainly study quaternary linear codes and their binary subfield codes. First we obtain a general explicit relationship between quaternary linear codes and their binary subfield codes in terms of generator matrices and defining sets. Second, we construct quaternary linear codes via simplicial complexes and determine the weight distributions of these codes. Third, the weight distributions of the binary subfield codes of these quaternary codes are also computed by employing the general characterization. Furthermore, we present two infinite families of optimal linear codes with respect to the Griesmer Bound, and a class of binary almost optimal codes with respect to the Sphere Packing Bound. We also need to emphasize that we obtain at least 9 new quaternary linear codes.</p></details> | <details><summary>24 pa...</summary><p>24 pages, to appear in IEEE TIT</p></details> |
| **[Lifted Reed-Solomon Codes with Application to Batch Codes](https://arxiv.org/pdf/2001.11981v2)** | 2020-02-05 | <details><summary>Show</summary><p>Guo, Kopparty and Sudan have initiated the study of error-correcting codes derived by lifting of affine-invariant codes. Lifted Reed-Solomon (RS) codes are defined as the evaluation of polynomials in a vector space over a field by requiring their restriction to every line in the space to be a codeword of the RS code. In this paper, we investigate lifted RS codes and discuss their application to batch codes, a notion introduced in the context of private information retrieval and load-balancing in distributed storage systems. First, we improve the estimate of the code rate of lifted RS codes for lifting parameter $m\ge 3$ and large field size. Second, a new explicit construction of batch codes utilizing lifted RS codes is proposed. For some parameter regimes, our codes have a better trade-off between parameters than previously known batch codes.</p></details> | <details><summary>7 pag...</summary><p>7 pages, 1 figure, a short version was submitted to ISIT 2020</p></details> |
| **[Grassmannian Codes with New Distance Measures for Network Coding](https://arxiv.org/pdf/1801.02329v7)** | 2019-02-11 | <details><summary>Show</summary><p>Grassmannian codes are known to be useful in error-correction for random network coding. Recently, they were used to prove that vector network codes outperform scalar linear network codes, on multicast networks, with respect to the alphabet size. The multicast networks which were used for this purpose are generalized combination networks. In both the scalar and the vector network coding solutions, the subspace distance is used as the distance measure for the codes which solve the network coding problem in the generalized combination networks. In this work we show that the subspace distance can be replaced with two other possible distance measures which generalize the subspace distance. These two distance measures are shown to be equivalent under an orthogonal transformation. It is proved that the Grassmannian codes with the new distance measures generalize the Grassmannian codes with the subspace distance and the subspace designs with the strength of the design. Furthermore, optimal Grassmannian codes with the new distance measureshave minimal requirements for network coding solutions of some generalized combination networks. The coding problems related to these two distance measures, especially with respect to network coding, are discussed. Finally, by using these new concepts it is proved that codes in the Hamming scheme form a subfamily of the Grassmannian codes.</p></details> |  |
| **[Coding Theorems for Repeat Multiple Accumulate Codes](https://arxiv.org/pdf/0810.3422v1)** | 2008-10-21 | <details><summary>Show</summary><p>In this paper the ensemble of codes formed by a serial concatenation of a repetition code with multiple accumulators connected through random interleavers is considered. Based on finite length weight enumerators for these codes, asymptotic expressions for the minimum distance and an arbitrary number of accumulators larger than one are derived using the uniform interleaver approach. In accordance with earlier results in the literature, it is first shown that the minimum distance of repeat-accumulate codes can grow, at best, sublinearly with block length. Then, for repeat-accumulate-accumulate codes and rates of 1/3 or less, it is proved that these codes exhibit asymptotically linear distance growth with block length, where the gap to the Gilbert-Varshamov bound can be made vanishingly small by increasing the number of accumulators beyond two. In order to address larger rates, random puncturing of a low-rate mother code is introduced. It is shown that in this case the resulting ensemble of repeat-accumulate-accumulate codes asymptotically achieves linear distance growth close to the Gilbert-Varshamov bound. This holds even for very high rate codes.</p></details> | <details><summary>Submi...</summary><p>Submitted to IEEE Transactions on Information Theory</p></details> |
| **[Constant-Rank Codes and Their Connection to Constant-Dimension Codes](https://arxiv.org/pdf/0803.2262v7)** | 2010-03-31 | <details><summary>Show</summary><p>Constant-dimension codes have recently received attention due to their significance to error control in noncoherent random linear network coding. What the maximal cardinality of any constant-dimension code with finite dimension and minimum distance is and how to construct the optimal constant-dimension code (or codes) that achieves the maximal cardinality both remain open research problems. In this paper, we introduce a new approach to solving these two problems. We first establish a connection between constant-rank codes and constant-dimension codes. Via this connection, we show that optimal constant-dimension codes correspond to optimal constant-rank codes over matrices with sufficiently many rows. As such, the two aforementioned problems are equivalent to determining the maximum cardinality of constant-rank codes and to constructing optimal constant-rank codes, respectively. To this end, we then derive bounds on the maximum cardinality of a constant-rank code with a given minimum rank distance, propose explicit constructions of optimal or asymptotically optimal constant-rank codes, and establish asymptotic bounds on the maximum rate of a constant-rank code.</p></details> | <details><summary>10 pa...</summary><p>10 pages, 3 figures, accepted to appear in IEEE Transactions on Information Theory</p></details> |
| **[New Counting Codes for Distributed Video Coding](https://arxiv.org/pdf/0710.0431v2)** | 2008-02-03 | <details><summary>Show</summary><p>This paper introduces a new counting code. Its design was motivated by distributed video coding where, for decoding, error correction methods are applied to improve predictions. Those error corrections sometimes fail which results in decoded values worse than the initial prediction. Our code exploits the fact that bit errors are relatively unlikely events: more than a few bit errors in a decoded pixel value are rare. With a carefully designed counting code combined with a prediction those bit errors can be corrected and sometimes the original pixel value recovered. The error correction improves significantly. Our new code not only maximizes the Hamming distance between adjacent (or "near 1") codewords but also between nearby (for example "near 2") codewords. This is why our code is significantly different from the well-known maximal counting sequences which have maximal average Hamming distance. Fortunately, the new counting code can be derived from Gray Codes for every code word length (i.e. bit depth).</p></details> | 10 pages, 4 tables |
| **[Cyclone Codes](https://arxiv.org/pdf/1605.00695v1)** | 2016-05-04 | <details><summary>Show</summary><p>We introduce Cyclone codes which are rateless erasure resilient codes. They combine Pair codes with Luby Transform (LT) codes by computing a code symbol from a random set of data symbols using bitwise XOR and cyclic shift operations. The number of data symbols is chosen according to the Robust Soliton distribution. XOR and cyclic shift operations establish a unitary commutative ring if data symbols have a length of $p-1$ bits, for some prime number $p$. We consider the graph given by code symbols combining two data symbols. If $n/2$ such random pairs are given for $n$ data symbols, then a giant component appears, which can be resolved in linear time. We can extend Cyclone codes to data symbols of arbitrary even length, provided the Goldbach conjecture holds. Applying results for this giant component, it follows that Cyclone codes have the same encoding and decoding time complexity as LT codes, while the overhead is upper-bounded by those of LT codes. Simulations indicate that Cyclone codes significantly decreases the overhead of extra coding symbols.</p></details> |  |
| **[Recommending Code Understandability Improvements based on Code Reviews](https://arxiv.org/pdf/2110.00782v1)** | 2021-10-05 | <details><summary>Show</summary><p>Developers spend 70% of their time understanding code. Code that is easy to read can save time, while hard-to-read code can lead to the introduction of bugs. However, it is difficult to establish what makes code more understandable. Although there are guides and directives on improving code understandability, in some contexts, these practices can have a detrimental effect. Practical software development projects often employ code review to improve code quality, including understandability. Reviewers are often senior developers who have contributed extensively to projects and have an in-depth understanding of the impacts of different solutions on code understandability. This paper is an early research proposal to recommend code understandability improvements based on code reviewer knowledge. The core of the proposal comprises a dataset of code understandability improvements extracted from code reviews. This dataset will serve as a basis to train machine learning systems to recommend understandability improvements.</p></details> |  |
| **[Senatus -- A Fast and Accurate Code-to-Code Recommendation Engine](https://arxiv.org/pdf/2111.04473v2)** | 2022-04-27 | <details><summary>Show</summary><p>Machine learning on source code (MLOnCode) is a popular research field that has been driven by the availability of large-scale code repositories and the development of powerful probabilistic and deep learning models for mining source code. Code-to-code recommendation is a task in MLOnCode that aims to recommend relevant, diverse and concise code snippets that usefully extend the code currently being written by a developer in their development environment (IDE). Code-to-code recommendation engines hold the promise of increasing developer productivity by reducing context switching from the IDE and increasing code-reuse. Existing code-to-code recommendation engines do not scale gracefully to large codebases, exhibiting a linear growth in query time as the code repository increases in size. In addition, existing code-to-code recommendation engines fail to account for the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leading to sub-optimal retrieval results. We address both of these weaknesses with \emph{Senatus}, a new code-to-code recommendation engine. At the core of Senatus is \emph{De-Skew} LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms. We evaluate Senatus and find the recommendations to be of higher quality than competing baselines, while achieving faster search. For example on the CodeSearchNet dataset Senatus improves performance by 31.21\% F1 and 147.9\emph{x} faster query time compared to Facebook Aroma. Senatus also outperforms standard MinHash LSH by 29.2\% F1 and 51.02\emph{x} faster query time.</p></details> | Accepted to MSR 2022 |
| **[Codes from Goppa codes](https://arxiv.org/pdf/2305.19565v5)** | 2024-03-19 | <details><summary>Show</summary><p>On a Goppa code whose structure polynomial has coefficients in the symbol field, the Frobenius acts. Its fixed codewords form a subcode. Deleting the naturally occurred redundance, we obtain a new code. It is proved that these new codes approach the Gilbert-Varshamov bound. It is also proved that these codes can be decoded within $O(n^2(\logn)^a)$ operations in the symbol field, which is usually much small than the location field, where $n$ is the codeword length, and $a$ a constant determined by the polynomial factorization algorithm.</p></details> | <details><summary>The a...</summary><p>The artical is reorganized</p></details> |
| **[An Optimal Linear Coding for Index Coding Problem](https://arxiv.org/pdf/1504.07570v2)** | 2015-05-08 | <details><summary>Show</summary><p>An optimal linear coding solution for index coding problem is established. Instead of network coding approach by focus on graph theoric and algebraic methods a linear coding program for solving both unicast and groupcast index coding problem is presented. The coding is proved to be the optimal solution from the linear perspective and can be easily utilize for any number of messages. The importance of this work is lying mostly on the usage of the presented coding in the groupcast index coding problem.</p></details> | 5 pages, 3 figures |
| **[New Linear Codes as Quasi-Twisted Codes from Long Constacyclic Codes](https://arxiv.org/pdf/2007.00604v1)** | 2021-06-25 | <details><summary>Show</summary><p>One of the most important and challenging problems in coding theory is to determine the optimal values of the parameters of a linear code and to explicitly construct codes with optimal parameters, or as close to the optimal values as possible. The class of quasi-twisted (QT) codes has been very promising in this regard. Over the past few decades various search algorithms to construct QT codes with better parameters have been employed. Most of these algorithms (such as ASR) start by joining constacyclic codes of smaller lengths to obtain QT codes of longer lengths. There has been an algorithm that works in the opposite way that constructs shorter QT codes from long constacyclic codes. We modified and generalized this algorithm and obtained new linear codes via its implementation. We also observe that the new algorithm is related to the ASR algorithm.</p></details> |  |
| **[Codes and Designs Related to Lifted MRD Codes](https://arxiv.org/pdf/1102.2593v5)** | 2016-11-17 | <details><summary>Show</summary><p>Lifted maximum rank distance (MRD) codes, which are constant dimension codes, are considered. It is shown that a lifted MRD code can be represented in such a way that it forms a block design known as a transversal design. A slightly different representation of this design makes it similar to a $q-$analog of a transversal design. The structure of these designs is used to obtain upper bounds on the sizes of constant dimension codes which contain a lifted MRD code. Codes which attain these bounds are constructed. These codes are the largest known codes for the given parameters. These transversal designs can be also used to derive a new family of linear codes in the Hamming space. Bounds on the minimum distance and the dimension of such codes are given.</p></details> | <details><summary>Submi...</summary><p>Submitted to IEEE Transactions on Information Theory. The material in this paper was presented in part in the 2011 IEEE International Symposium on Information Theory, Saint Petersburg, Russia, August 2011</p></details> |
| **[Performance of Polar Codes for Channel and Source Coding](https://arxiv.org/pdf/0901.2370v2)** | 2009-05-22 | <details><summary>Show</summary><p>Polar codes, introduced recently by Arıkan, are the first family of codes known to achieve capacity of symmetric channels using a low complexity successive cancellation decoder. Although these codes, combined with successive cancellation, are optimal in this respect, their finite-length performance is not record breaking. We discuss several techniques through which their finite-length performance can be improved. We also study the performance of these codes in the context of source coding, both lossless and lossy, in the single-user context as well as for distributed applications.</p></details> | <details><summary>accep...</summary><p>accepted in ISIT 2009, Figure 5 is different from the previous version</p></details> |
| **[Group code structures on affine-invariant codes](https://arxiv.org/pdf/0903.1033v1)** | 2009-03-06 | <details><summary>Show</summary><p>A group code structure of a linear code is a description of the code as one-sided or two-sided ideal of a group algebra of a finite group. In these realizations, the group algebra is identified with the ambient space, and the group elements with the coordinates of the ambient space. It is well known that every affine-invariant code of length $p^m$, with $p$ prime, can be realized as an ideal of the group algebra $\F\I$, where $\I$ is the underlying additive group of the field with $p^m$ elements. In this paper we describe all the group code structures of an affine-invariant code of length $p^m$ in terms of a family of maps from $\I$ to the group of automorphisms of $\I$.</p></details> | 7 pages |
| **[Space-Time Codes from Sum-Rank Codes](https://arxiv.org/pdf/2103.04976v1)** | 2021-03-10 | <details><summary>Show</summary><p>Just as rank-metric or Gabidulin codes may be used to construct rate-diversity tradeoff optimal space-time codes, a recently introduced generalization for the sum-rank metric -- linearized Reed-Solomon codes -- accomplishes the same in the case of multiple fading blocks. In this paper, we provide the first explicit construction of minimal delay rate-diversity optimal multiblock space-time codes as an application of linearized Reed-Solomon codes. We also provide sequential decoders for these codes and, more generally, space-time codes constructed from finite field codes. Simulation results show that the proposed codes can outperform full diversity codes based on cyclic division algebras at low SNRs as well as utilize significantly smaller constellations.</p></details> | <details><summary>Submi...</summary><p>Submitted to IEEE Transactions on Information Theory</p></details> |
| **[Aroma: Code Recommendation via Structural Code Search](https://arxiv.org/pdf/1812.01158v4)** | 2019-10-21 | <details><summary>Show</summary><p>Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an IDE plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently.</p></details> |  |
| **[Quantum Codes from Generalized Reed-Solomon Codes and Matrix-Product Codes](https://arxiv.org/pdf/1508.00978v1)** | 2015-08-06 | <details><summary>Show</summary><p>One of the central tasks in quantum error-correction is to construct quantum codes that have good parameters. In this paper, we construct three new classes of quantum MDS codes from classical Hermitian self-orthogonal generalized Reed-Solomon codes. We also present some classes of quantum codes from matrix-product codes. It turns out that many of our quantum codes are new in the sense that the parameters of quantum codes cannot be obtained from all previous constructions.</p></details> |  |
| **[Generalized Simple Streaming Codes from MDS Codes](https://arxiv.org/pdf/2104.07005v1)** | 2021-04-15 | <details><summary>Show</summary><p>Streaming codes represent a packet-level FEC scheme for achieving reliable, low-latency communication. In the literature on streaming codes, the commonly-assumed Gilbert-Elliott channel model, is replaced by a more tractable, delay-constrained, sliding-window (DCSW) channel model that can introduce either random or burst erasures. The known streaming codes that are rate optimal over the DCSW channel model are constructed by diagonally embedding a scalar block code across successive packets. These code constructions have field size that is quadratic in the delay parameter $τ$ and have a somewhat complex structure with an involved decoding procedure. This led to the introduction of simple streaming (SS) codes in which diagonal embedding is replaced by staggered-diagonal embedding (SDE). The SDE approach reduces the impact of a burst of erasures and makes it possible to construct near-rate-optimal streaming codes using Maximum Distance Separable (MDS) code having linear field size. The present paper takes this development one step further, by retaining the staggered-diagonal feature, but permitting the placement of more than one code symbol from a given scalar codeword within each packet. These generalized, simple streaming codes allow us to improve upon the rate of SS codes, while retaining the simplicity of working with MDS codes. We characterize the maximum code rate of streaming codes under a constraint on the number of contiguous packets over which symbols of the underlying scalar code are dispersed. Such a constraint leads to simplified code construction and reduced-complexity decoding.</p></details> |  |
| **[Evaluation of Code LLMs on Geospatial Code Generation](https://arxiv.org/pdf/2410.04617v2)** | 2024-12-16 | <details><summary>Show</summary><p>Software development support tools have been studied for a long time, with recent approaches using Large Language Models (LLMs) for code generation. These models can generate Python code for data science and machine learning applications. LLMs are helpful for software engineers because they increase productivity in daily work. An LLM can also serve as a "mentor" for inexperienced software developers, and be a viable learning support. High-quality code generation with LLMs can also be beneficial in geospatial data science. However, this domain poses different challenges, and code generation LLMs are typically not evaluated on geospatial tasks. Here, we show how we constructed an evaluation benchmark for code generation models, based on a selection of geospatial tasks. We categorised geospatial tasks based on their complexity and required tools. Then, we created a dataset with tasks that test model capabilities in spatial reasoning, spatial data processing, and geospatial tools usage. The dataset consists of specific coding problems that were manually created for high quality. For every problem, we proposed a set of test scenarios that make it possible to automatically check the generated code for correctness. In addition, we tested a selection of existing code generation LLMs for code generation in the geospatial domain. We share our dataset and reproducible evaluation code on a public GitHub repository, arguing that this can serve as an evaluation benchmark for new LLMs in the future. Our dataset will hopefully contribute to the development new models capable of solving geospatial coding tasks with high accuracy. These models will enable the creation of coding assistants tailored for geospatial applications.</p></details> | <details><summary>7th A...</summary><p>7th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery (GeoAI'24)</p></details> |
| **[Coding Theorem for Generalized Reed-Solomon Codes](https://arxiv.org/pdf/2505.08326v1)** | 2025-05-14 | <details><summary>Show</summary><p>In this paper, we prove that the sub-field images of generalized Reed-Solomon (RS) codes can achieve the symmetric capacity of p-ary memoryless channels. Unlike the totally random linear code ensemble, as a class of maximum distance separable (MDS) codes, the generalized RS code ensemble lacks the pair-wise independence among codewords and has non-identical distributions of nonzero codewords. To prove the coding theorem for the p-ary images of generalized RS codes, we analyze the exponential upper bound on the error probability of the generalized RS code in terms of its spectrum using random coding techniques. In the finite-length region, we present an ML decoding algorithm for the generalized RS codes over the binary erasure channels (BECs). In particular, the algebraic structure of the generalized RS codes allows us to implement the parallel Lagrange interpolation to derive an ordered systematic matrix. Subsequently, we can reconstruct the ML codeword through a change of basis, accelerating the conventional Gaussian elimination (GE), as validated in the simulation results. Additionally, we apply this decoding technique to the LC-OSD algorithm over the additive white Gaussian noise (AWGN) channels with binary phase shift keying (BPSK) modulation and three-level pulse amplitude modulation (3PAM). Simulation results show that, in the high-rate region, generalized RS codes defined over fields of characteristic three with 3-PAM perform better than those defined over fields of characteristic two with BPSK.</p></details> | 26 pages, 10 figures |
| **[Two classes of subfield codes of linear codes](https://arxiv.org/pdf/2211.00426v1)** | 2022-11-02 | <details><summary>Show</summary><p>Recently, subfiled codes of linear code over GF$ (q) $ with good parameters were studied, and many optimal subfield codes were obtained. In this paper, Our mainly motivation is to generlize the results of the subfield codes of hyperoval in Ding and Heng (Finite Fields Their Appl. 56, 308-331 (2019)), and generlize the results of two families of subfield codes in Xiang and Yin (Cryptogr. Commun. 13(1), 117-127 (2021)) to $ p $-ary where $ p $ is odd. We get the parameters and weight distribution of these subfield codes. At the same time, the parameters of their dual codes are also studied. When $ m=1 $, The dual codes of these subfield codes are almost MDS code, when $ m>1 $ and $ p $ odd, these dual codes are dimension-optimal with respect to the sphere-backing bound.</p></details> |  |
| **[Reliability on QR codes and Reed-Solomon codes](https://arxiv.org/pdf/2407.17364v1)** | 2024-07-25 | <details><summary>Show</summary><p>This study addresses the use of Reed-Solomon error correction codes in QR codes to enhance resilience against failures. To fully grasp this approach, a basic cryptographic context is provided, necessary for understanding Reed-Solomon codes. The study begins by defining a code and explores key outcomes for codes with additional properties, such as linearity. The theoretical framework is further developed with specific definitions and examples of Reed-Solomon codes, presented as a particular variant of BCH codes. Additionally, the structure of QR codes is analyzed, encompassing different versions and how data is represented in the form of black and white pixels within an image. Finally, an inherent vulnerability of Reed-Solomon Codes, and particularly of QR codes, related to selective manipulation of modules is examined. This vulnerability leverages the error correction mechanisms present in Reed-Solomon codes.</p></details> |  |
| **[Construction for both self-dual codes and LCD codes](https://arxiv.org/pdf/2108.12544v1)** | 2021-08-31 | <details><summary>Show</summary><p>From a given $[n, k]$ code $C$, we give a method for constructing many $[n, k]$ codes $C'$ such that the hull dimensions of $C$ and $C'$ are identical. This method can be applied to constructions of both self-dual codes and linear complementary dual codes (LCD codes for short). Using the method, we construct 661 new inequivalent extremal doubly even $[56, 28, 12]$ codes. Furthermore, constructing LCD codes by the method, we improve some of the previously known lower bounds on the largest minimum weights of binary LCD codes of length $n=26,28 \le n \le 40$.</p></details> |  |
| **[PAC Codes for Source and Joint Source-Channel Coding](https://arxiv.org/pdf/2308.05472v1)** | 2023-08-11 | <details><summary>Show</summary><p>Polarization-adjusted convolutional (PAC) codes, as a concatenated coding scheme based on polar codes, is able to approach the finite-length bound of binary-input AWGN channel at short blocklengths. In this paper, we extend PAC codes to the fields of source coding and joint source-channel coding and show that they can also approach the corresponding finite-length bounds at short blocklengths.</p></details> | <details><summary>6 pag...</summary><p>6 pages, 6 figures. Submitted to GC 2023 Workshop - Channel Coding Beyond 5G</p></details> |
| **[List Decoding of Matrix-Product Codes from nested codes: an application to Quasi-Cyclic codes](https://arxiv.org/pdf/1201.6397v1)** | 2013-04-15 | <details><summary>Show</summary><p>A list decoding algorithm for matrix-product codes is provided when $C_1,..., C_s$ are nested linear codes and $A$ is a non-singular by columns matrix. We estimate the probability of getting more than one codeword as output when the constituent codes are Reed-Solomon codes. We extend this list decoding algorithm for matrix-product codes with polynomial units, which are quasi-cyclic codes. Furthermore, it allows us to consider unique decoding for matrix-product codes with polynomial units.</p></details> |  |
| **[Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs](https://arxiv.org/pdf/2502.19411v1)** | 2025-02-27 | <details><summary>Show</summary><p>In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.</p></details> | <details><summary>Proje...</summary><p>Project Repo: https://github.com/dayuyang1999/Awesome-Code-Reasoning</p></details> |

## Program
| **Title** | **Date** | **Abstract** | **Comment** |
| --- | --- | --- | --- |
| **[Program Analysis of Probabilistic Programs](https://arxiv.org/pdf/2204.06868v1)** | 2022-04-15 | <details><summary>Show</summary><p>Probabilistic programming is a growing area that strives to make statistical analysis more accessible, by separating probabilistic modelling from probabilistic inference. In practice this decoupling is difficult. No single inference algorithm can be used as a probabilistic programming back-end that is simultaneously reliable, efficient, black-box, and general. Probabilistic programming languages often choose a single algorithm to apply to a given problem, thus inheriting its limitations. While substantial work has been done both to formalise probabilistic programming and to improve efficiency of inference, there has been little work that makes use of the available program structure, by formally analysing it, to better utilise the underlying inference algorithm. This dissertation presents three novel techniques (both static and dynamic), which aim to improve probabilistic programming using program analysis. The techniques analyse a probabilistic program and adapt it to make inference more efficient, sometimes in a way that would have been tedious or impossible to do by hand.</p></details> | PhD thesis |
| **[Programming with Neural Surrogates of Programs](https://arxiv.org/pdf/2112.06148v1)** | 2021-12-14 | <details><summary>Show</summary><p>Surrogates, models that mimic the behavior of programs, form the basis of a variety of development workflows. We study three surrogate-based design patterns, evaluating each in case studies on a large-scale CPU simulator. With surrogate compilation, programmers develop a surrogate that mimics the behavior of a program to deploy to end-users in place of the original program. Surrogate compilation accelerates the CPU simulator under study by $1.6\times$. With surrogate adaptation, programmers develop a surrogate of a program then retrain that surrogate on a different task. Surrogate adaptation decreases the simulator's error by up to $50\%$. With surrogate optimization, programmers develop a surrogate of a program, optimize input parameters of the surrogate, then plug the optimized input parameters back into the original program. Surrogate optimization finds simulation parameters that decrease the simulator's error by $5\%$ compared to the error induced by expert-set parameters. In this paper we formalize this taxonomy of surrogate-based design patterns. We further describe the programming methodology common to all three design patterns. Our work builds a foundation for the emerging class of workflows based on programming with surrogates of programs.</p></details> |  |
| **[Programs Versus Finite Tree-Programs](https://arxiv.org/pdf/2501.01820v1)** | 2025-01-06 | <details><summary>Show</summary><p>In this paper, we study classes of structures and individual structures for which programs implementing functions defined everywhere are equivalent to finite tree-programs. The programs under consideration may have cycles and at most countably many nodes. We start with programs in which arbitrary terms of a given signature may be used in function nodes and arbitrary formulas of this signature may be used in predicate nodes. We then extend our results to programs that are close in nature to computation trees: if such a program is a finite tree-program, then it is an ordinary computation tree.</p></details> |  |
| **[Programming-By-Example by Programming-By-Example: Synthesis of Looping Programs](https://arxiv.org/pdf/2108.08724v1)** | 2021-08-20 | <details><summary>Show</summary><p>Program synthesis has seen many new applications in recent years, in large part thanks to the introduction of SyGuS. However, no existing SyGuS solvers have support for synthesizing recursive functions. We introduce an multi-phase algorithm for the synthesis of recursive ``looplike'' programs in SyGuS for programming-by-example. We solve constraints individually and treat them as ``unrolled`` examples of how a recursive program would behave, and solve for the generalized recursive solution. Our approach is modular and supports any SyGuS Solver.</p></details> |  |
| **[Dynamic Neural Program Embedding for Program Repair](https://arxiv.org/pdf/1711.07163v4)** | 2018-07-03 | <details><summary>Show</summary><p>Neural program embeddings have shown much promise recently for a variety of program analysis tasks, including program synthesis, program repair, fault localization, etc. However, most existing program embeddings are based on syntactic features of programs, such as raw token sequences or abstract syntax trees. Unlike images and text, a program has an unambiguous semantic meaning that can be difficult to capture by only considering its syntax (i.e. syntactically similar pro- grams can exhibit vastly different run-time behavior), which makes syntax-based program embeddings fundamentally limited. This paper proposes a novel semantic program embedding that is learned from program execution traces. Our key insight is that program states expressed as sequential tuples of live variable values not only captures program semantics more precisely, but also offer a more natural fit for Recurrent Neural Networks to model. We evaluate different syntactic and semantic program embeddings on predicting the types of errors that students make in their submissions to an introductory programming class and two exercises on the CodeHunt education platform. Evaluation results show that our new semantic program embedding significantly outperforms the syntactic program embeddings based on token sequences and abstract syntax trees. In addition, we augment a search-based program repair system with the predictions obtained from our se- mantic embedding, and show that search efficiency is also significantly improved.</p></details> | 9 pages |
| **[Programming for All: Understanding the Nature of Programs](https://arxiv.org/pdf/2111.04887v2)** | 2021-12-07 | <details><summary>Show</summary><p>Computer programs are part of our daily life, we use them, we provide them with data, they support our decisions, they help us remember, they control machines, etc. Programs are made by people, but in most cases we are not their authors, so we have to decide if we can trust them. Programs enable computers and computer-controlled machines to behave in a large variety of ways. They bring the intrinsic power of computers to life. Programs have a variety of properties that all citizens must be aware of. Due to the intangible nature of programs, most of these properties are very unusual, but important to understand the digital world. In this position paper, we describe the Nature of Programs in the form of knowledge statements, accompanied by examples from everyday life to clarify their meaning. Everything is formulated in an easily understandable manner and avoids obscure technical language. We suggest that these knowledge statements must be imparted to all teachers and school students. A great way to learn and experience the nature of programs is to develop programs yourself.</p></details> |  |
| **[The meaning of a program change is a change to the program's meaning](https://arxiv.org/pdf/1908.00898v1)** | 2019-08-05 | <details><summary>Show</summary><p>Programming is the activity of modifying a program in order to bring about specific changes in its behaviour. Yet programming language theory almost exclusively focuses on the meaning of programs. We motivate a "change-oriented" viewpoint from which the meaning of a program change is a change to the program's meaning.</p></details> |  |
| **[onlineSPARC: a Programming Environment for Answer Set Programming](https://arxiv.org/pdf/1809.08304v1)** | 2018-09-25 | <details><summary>Show</summary><p>Recent progress in logic programming (e.g., the development of the Answer Set Programming paradigm) has made it possible to teach it to general undergraduate and even middle/high school students. Given the limited exposure of these students to computer science, the complexity of downloading, installing and using tools for writing logic programs could be a major barrier for logic programming to reach a much wider audience. We developed onlineSPARC, an online answer set programming environment with a self contained file system and a simple interface. It allows users to type/edit logic programs and perform several tasks over programs, including asking a query to a program, getting the answer sets of a program, and producing a drawing/animation based on the answer sets of a program.</p></details> | <details><summary>Under...</summary><p>Under consideration in Theory and Practice of Logic Programming (TPLP)</p></details> |
| **[Modelling Program Spaces in Program Synthesis with Constraints](https://arxiv.org/pdf/2508.00005v1)** | 2025-08-04 | <details><summary>Show</summary><p>A core challenge in program synthesis is taming the large space of possible programs. Since program synthesis is essentially a combinatorial search, the community has sought to leverage powerful combinatorial constraint solvers. Here, constraints are used to express the program semantics, but not as a potentially potent tool to remove unwanted programs. Recent inductive logic programming approaches introduce constraints on the program's syntax to be synthesized. These syntactic constraints allow for checking and propagating a constraint without executing the program, and thus for arbitrary operators. In this work, we leverage syntactic constraints to model program spaces, defining not just solutions that are feasible, but also ones that are likely useful. To demonstrate this idea, we introduce BART, a solver that efficiently propagates and solves these constraints. We evaluate BART on program space enumeration tasks, finding that the constraints eliminate up to 99 percent of the program space, and that modeling program spaces significantly reduces enumeration time.</p></details> |  |
| **[Formulas as Programs](https://arxiv.org/pdf/cs/9811017v1)** | 2005-09-17 | <details><summary>Show</summary><p>We provide here a computational interpretation of first-order logic based on a constructive interpretation of satisfiability w.r.t. a fixed but arbitrary interpretation. In this approach the formulas themselves are programs. This contrasts with the so-called formulas as types approach in which the proofs of the formulas are typed terms that can be taken as programs. This view of computing is inspired by logic programming and constraint logic programming but differs from them in a number of crucial aspects. Formulas as programs is argued to yield a realistic approach to programming that has been realized in the implemented programming language ALMA-0 (Apt et al.) that combines the advantages of imperative and logic programming. The work here reported can also be used to reason about the correctness of non-recursive ALMA-0 programs that do not include destructive assignment.</p></details> | <details><summary>34 pa...</summary><p>34 pages, appears in: The Logic Programming Paradigm: a 25 Years Perspective, K.R. Apt, V. Marek, M. Truszczynski and D.S. Warren (eds), Springer-Verlag, Artificial Intelligence Series</p></details> |
| **[Learning logic programs by combining programs](https://arxiv.org/pdf/2206.01614v3)** | 2023-08-21 | <details><summary>Show</summary><p>The goal of inductive logic programming is to induce a logic program (a set of logical rules) that generalises training examples. Inducing programs with many rules and literals is a major challenge. To tackle this challenge, we introduce an approach where we learn small non-separable programs and combine them. We implement our approach in a constraint-driven ILP system. Our approach can learn optimal and recursive programs and perform predicate invention. Our experiments on multiple domains, including game playing and program synthesis, show that our approach can drastically outperform existing approaches in terms of predictive accuracies and learning times, sometimes reducing learning times from over an hour to a few seconds.</p></details> |  |
| **[Tensor Program Optimization with Probabilistic Programs](https://arxiv.org/pdf/2205.13603v2)** | 2022-10-11 | <details><summary>Show</summary><p>Automatic optimization for tensor programs becomes increasingly important as we deploy deep learning in various environments, and efficient optimization relies on a rich search space and effective search. Most existing efforts adopt a search space which lacks the ability to efficiently enable domain experts to grow the search space. This paper introduces MetaSchedule, a domain-specific probabilistic programming language abstraction to construct a rich search space of tensor programs. Our abstraction allows domain experts to analyze the program, and easily propose stochastic choices in a modular way to compose program transformation accordingly. We also build an end-to-end learning-driven framework to find an optimized program for a given search space. Experimental results show that MetaSchedule can cover the search space used in the state-of-the-art tensor program optimization frameworks in a modular way. Additionally, it empowers domain experts to conveniently grow the search space and modularly enhance the system, which brings 48% speedup on end-to-end deep learning workloads.</p></details> | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2022</p></details> |
| **[Reflection and Hyper-Programming in Persistent Programming Systems](https://arxiv.org/pdf/1006.3481v1)** | 2010-06-18 | <details><summary>Show</summary><p>The work presented in this thesis seeks to improve programmer productivity in the following ways: - by reducing the amount of code that has to be written to construct an application; - by increasing the reliability of the code written; and - by improving the programmer's understanding of the persistent environment in which applications are constructed. Two programming techniques that may be used to pursue these goals in a persistent environment are type-safe linguistic reflection and hyper-programming. The first provides a mechanism by which the programmer can write generators that, when executed, produce new program representations. This allows the specification of programs that are highly generic yet depend in non-trivial ways on the types of the data on which they operate. Genericity promotes software reuse which in turn reduces the amount of new code that has to be written. Hyper-programming allows a source program to contain links to data items in the persistent store. This improves program reliability by allowing certain program checking to be performed earlier than is otherwise possible. It also reduces the amount of code written by permitting direct links to data in the place of textual descriptions. Both techniques contribute to the understanding of the persistent environment through supporting the implementation of store browsing tools and allowing source representations to be associated with all executable programs in the persistent store. This thesis describes in detail the structure of type-safe linguistic reflection and hyper-programming, their benefits in the persistent context, and a suite of programming tools that support reflective programming and hyper-programming. These tools may be used in conjunction to allow reflection over hyper-program representations. The implementation of the tools is described.</p></details> | <details><summary>PhD T...</summary><p>PhD Thesis, University of St Andrews. Supervisor: R. Morrison. (1992)</p></details> |
| **[Newtonian Program Analysis of Probabilistic Programs](https://arxiv.org/pdf/2307.09064v2)** | 2024-03-08 | <details><summary>Show</summary><p>Due to their quantitative nature, probabilistic programs pose non-trivial challenges for designing compositional and efficient program analyses. Many analyses for probabilistic programs rely on iterative approximation. This article presents an interprocedural dataflow-analysis framework, called NPA-PMA, for designing and implementing (partially) non-iterative program analyses of probabilistic programs with unstructured control-flow, nondeterminism, and general recursion. NPA-PMA is based on Newtonian Program Analysis (NPA), a generalization of Newton's method to solve equation systems over semirings. The key challenge for developing NPA-PMA is to handle multiple kinds of confluences in both the algebraic structures that specify analyses and the equation systems that encode control flow: semirings support a single confluence operation, whereas NPA-PMA involves three confluence operations (conditional, probabilistic, and nondeterministic). Our work introduces $ω$-continuous pre-Markov algebras ($ω$PMAs) to factor out common parts of different analyses; adopts regular infinite-tree expressions to encode program-execution paths in control-flow hyper-graphs; and presents a linearization method that makes Newton's method applicable to the setting of regular-infinite-tree equations over $ω$PMAs. NPA-PMA allows analyses to supply a non-iterative strategy to solve linearized equations. Our experimental evaluation demonstrates that (i) NPA-PMA holds considerable promise for outperforming Kleene iteration, and (ii) provides great generality for designing program analyses.</p></details> |  |
| **[ProgramAlly: Creating Custom Visual Access Programs via Multi-Modal End-User Programming](https://arxiv.org/pdf/2408.10499v1)** | 2024-08-21 | <details><summary>Show</summary><p>Existing visual assistive technologies are built for simple and common use cases, and have few avenues for blind people to customize their functionalities. Drawing from prior work on DIY assistive technology, this paper investigates end-user programming as a means for users to create and customize visual access programs to meet their unique needs. We introduce ProgramAlly, a system for creating custom filters for visual information, e.g., 'find NUMBER on BUS', leveraging three end-user programming approaches: block programming, natural language, and programming by example. To implement ProgramAlly, we designed a representation of visual filtering tasks based on scenarios encountered by blind people, and integrated a set of on-device and cloud models for generating and running these programs. In user studies with 12 blind adults, we found that participants preferred different programming modalities depending on the task, and envisioned using visual access programs to address unique accessibility challenges that are otherwise difficult with existing applications. Through ProgramAlly, we present an exploration of how blind end-users can create visual access programs to customize and control their experiences.</p></details> | UIST 2024 |
| **[Transforming Coroutining Logic Programs into Equivalent CHR Programs](https://arxiv.org/pdf/1708.07222v1)** | 2017-08-28 | <details><summary>Show</summary><p>We extend a technique called Compiling Control. The technique transforms coroutining logic programs into logic programs that, when executed under the standard left-to-right selection rule (and not using any delay features) have the same computational behavior as the coroutining program. In recent work, we revised Compiling Control and reformulated it as an instance of Abstract Conjunctive Partial Deduction. This work was mostly focused on the program analysis performed in Compiling Control. In the current paper, we focus on the synthesis of the transformed program. Instead of synthesizing a new logic program, we synthesize a CHR(Prolog) program which mimics the coroutining program. The synthesis to CHR yields programs containing only simplification rules, which are particularly amenable to certain static analysis techniques. The programs are also more concise and readable and can be ported to CHR implementations embedded in other languages than Prolog.</p></details> | <details><summary>In Pr...</summary><p>In Proceedings VPT 2017, arXiv:1708.06887</p></details> |
| **[Translating Xd-C programs to MSVL programs](https://arxiv.org/pdf/1809.00959v2)** | 2019-01-23 | <details><summary>Show</summary><p>C language is one of the most popular languages for software systems. In order to verify safety, reliability and security properties of such systems written in C, a tool UMC4M for runtime verification at code level based on Modeling, Simulation and Verification Language (MSVL) and its compiler MC is employed. To do so, a C program $P$ has to be translated to an MSVL program M and the negation of a desired property $Q$ is also translated to an MSVL program M', then "M and M" is compiled and executed armed with MC. Whether $P$ violates $Q$ is checked by evaluating whether there exists an acceptable execution of new MSVL program M and M". Therefore, how to translate a C program to an MSVL program is a critical issue. However, in general, C is of complicated structures with goto statement. In this paper, we confine the syntax of C in a suitable subset called Xd-C without loss of expressiveness. Further, we present a translation algorithm from an Xd-C program to an MSVL program based on translation algorithms for expressions and statements. Moreover, the equivalences between expressions and statements involved in Xd-C and MSVL programs are inductively proved. Subsequently, the equivalence between the original Xd-C program and the translated MSVL program is also proved. In addition, the proposed approach has been implemented by a tool called $C2M$. A benchmark of experiments including 13 real-world Xd-C programs is conducted. The results show that $C2M$ works effectively.</p></details> |  |
| **[TerpreT: A Probabilistic Programming Language for Program Induction](https://arxiv.org/pdf/1608.04428v1)** | 2016-08-17 | <details><summary>Show</summary><p>We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs. Our aims are to develop new machine learning approaches based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programming languages community. Our key contribution is the proposal of TerpreT, a domain-specific language for expressing program synthesis problems. TerpreT is similar to a probabilistic programming language: a model is composed of a specification of a program representation (declarations of random variables) and an interpreter describing how programs map inputs to outputs (a model connecting unknowns to observations). The inference task is to observe a set of input-output examples and infer the underlying program. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing like-to-like comparisons between different approaches to inference. From a single TerpreT specification we automatically perform inference using four different back-ends. These are based on gradient descent, linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. We illustrate the value of TerpreT by developing several interpreter models and performing an empirical comparison between alternative inference algorithms. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations. We conclude with suggestions for the machine learning community to make progress on program synthesis.</p></details> | <details><summary>50 pa...</summary><p>50 pages, 20 figures, 4 tables</p></details> |
| **[Program Skeletons for Automated Program Translation](https://arxiv.org/pdf/2504.07483v2)** | 2025-09-16 | <details><summary>Show</summary><p>Translating software between programming languages is a challenging task, for which automated techniques have been elusive and hard to scale up to larger programs. A key difficulty in cross-language translation is that one has to re-express the intended behavior of the source program into idiomatic constructs of a different target language. This task needs abstracting away from the source language-specific details, while keeping the overall functionality the same. In this work, we propose a novel and systematic approach for making such translation amenable to automation based on a framework we call program skeletons. A program skeleton retains the high-level structure of the source program by abstracting away and effectively summarizing lower-level concrete code fragments, which can be mechanically translated to the target programming language. A skeleton, by design, permits many different ways of filling in the concrete implementation for fragments, which can work in conjunction with existing data-driven code synthesizers. Most importantly, skeletons can conceptually enable sound decomposition, i.e., if each individual fragment is correctly translated, taken together with the mechanically translated skeleton, the final translated program is deemed to be correct as a whole. We present a prototype system called Skel embodying the idea of skeleton-based translation from Python to JavaScript. Our results show promising scalability compared to prior works. For 9 real-world Python programs, some with more than about 1k lines of code, 95% of their code fragments can be automatically translated, while about 5% require manual effort. All the final translations are correct with respect to whole-program test suites.</p></details> | <details><summary>Accep...</summary><p>Accepted by PLDI 2025 (46th ACM SIGPLAN Conference on Programming Language Design and Implementation)</p></details> |
| **[Relativized hyperequivalence of logic programs for modular programming](https://arxiv.org/pdf/0907.4128v1)** | 2009-07-24 | <details><summary>Show</summary><p>A recent framework of relativized hyperequivalence of programs offers a unifying generalization of strong and uniform equivalence. It seems to be especially well suited for applications in program optimization and modular programming due to its flexibility that allows us to restrict, independently of each other, the head and body alphabets in context programs. We study relativized hyperequivalence for the three semantics of logic programs given by stable, supported and supported minimal models. For each semantics, we identify four types of contexts, depending on whether the head and body alphabets are given directly or as the complement of a given set. Hyperequivalence relative to contexts where the head and body alphabets are specified directly has been studied before. In this paper, we establish the complexity of deciding relativized hyperequivalence with respect to the three other types of context programs. To appear in Theory and Practice of Logic Programming (TPLP).</p></details> |  |
| **[Open Answer Set Programming with Guarded Programs](https://arxiv.org/pdf/cs/0603025v2)** | 2007-02-25 | <details><summary>Show</summary><p>Open answer set programming (OASP) is an extension of answer set programming where one may ground a program with an arbitrary superset of the program's constants. We define a fixed point logic (FPL) extension of Clark's completion such that open answer sets correspond to models of FPL formulas and identify a syntactic subclass of programs, called (loosely) guarded programs. Whereas reasoning with general programs in OASP is undecidable, the FPL translation of (loosely) guarded programs falls in the decidable (loosely) guarded fixed point logic (mu(L)GF). Moreover, we reduce normal closed ASP to loosely guarded OASP, enabling for the first time, a characterization of an answer set semantics by muLGF formulas. We further extend the open answer set semantics for programs with generalized literals. Such generalized programs (gPs) have interesting properties, e.g., the ability to express infinity axioms. We restrict the syntax of gPs such that both rules and generalized literals are guarded. Via a translation to guarded fixed point logic, we deduce 2-exptime-completeness of satisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted GgPs with exptime-complete satisfiability checking, but still sufficiently expressive to optimally simulate computation tree logic (CTL). We translate Datalog lite programs to GgPs, establishing equivalence of GgPs under an open answer set semantics, alternation-free muGF, and Datalog lite.</p></details> | <details><summary>51 pa...</summary><p>51 pages, 1 figure, accepted for publication in ACM's TOCL</p></details> |
| **[Inducing Probabilistic Programs by Bayesian Program Merging](https://arxiv.org/pdf/1110.5667v1)** | 2011-10-27 | <details><summary>Show</summary><p>This report outlines an approach to learning generative models from data. We express models as probabilistic programs, which allows us to capture abstract patterns within the examples. By choosing our language for programs to be an extension of the algebraic data type of the examples, we can begin with a program that generates all and only the examples. We then introduce greater abstraction, and hence generalization, incrementally to the extent that it improves the posterior probability of the examples given the program. Motivated by previous approaches to model merging and program induction, we search for such explanatory abstractions using program transformations. We consider two types of transformation: Abstraction merges common subexpressions within a program into new functions (a form of anti-unification). Deargumentation simplifies functions by reducing the number of arguments. We demonstrate that this approach finds key patterns in the domain of nested lists, including parameterized sub-functions and stochastic recursion.</p></details> |  |
| **[Object-Oriented Programming, Functional Programming and R](https://arxiv.org/pdf/1409.3531v1)** | 2014-09-12 | <details><summary>Show</summary><p>This paper reviews some programming techniques in R that have proved useful, particularly for substantial projects. These include several versions of object-oriented programming, used in a large number of R packages. The review tries to clarify the origins and ideas behind the various versions, each of which is valuable in the appropriate context. R has also been strongly influenced by the ideas of functional programming and, in particular, by the desire to combine functional with object oriented programming. To clarify how this particular mix of ideas has turned out in the current R language and supporting software, the paper will first review the basic ideas behind object-oriented and functional programming, and then examine the evolution of R with these ideas providing context. Functional programming supports well-defined, defensible software giving reproducible results. Object-oriented programming is the mechanism par excellence for managing complexity while keeping things simple for the user. The two paradigms have been valuable in supporting major software for fitting models to data and numerous other statistical applications. The paradigms have been adopted, and adapted, distinctively in R. Functional programming motivates much of R but R does not enforce the paradigm. Object-oriented programming from a functional perspective differs from that used in non-functional languages, a distinction that needs to be emphasized to avoid confusion. R initially replicated the S language from Bell Labs, which in turn was strongly influenced by earlier program libraries. At each stage, new ideas have been added, but the previous software continues to show its influence in the design as well. Outlining the evolution will further clarify why we currently have this somewhat unusual combination of ideas.</p></details> | <details><summary>Publi...</summary><p>Published in at http://dx.doi.org/10.1214/13-STS452 the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org)</p></details> |
| **[ProTo: Program-Guided Transformer for Program-Guided Tasks](https://arxiv.org/pdf/2110.00804v2)** | 2021-10-19 | <details><summary>Show</summary><p>Programs, consisting of semantic and structural information, play an important role in the communication between humans and agents. Towards learning general program executors to unify perception, reasoning, and decision making, we formulate program-guided tasks which require learning to execute a given program on the observed task specification. Furthermore, we propose the Program-guided Transformer (ProTo), which integrates both semantic and structural guidance of a program by leveraging cross-attention and masked self-attention to pass messages between the specification and routines in the program. ProTo executes a program in a learned latent space and enjoys stronger representation ability than previous neural-symbolic approaches. We demonstrate that ProTo significantly outperforms the previous state-of-the-art methods on GQA visual reasoning and 2D Minecraft policy learning datasets. Additionally, ProTo demonstrates better generalization to unseen, complex, and human-written programs.</p></details> | <details><summary>Accep...</summary><p>Accepted in NeurIPS 2021</p></details> |
| **[Theory of Programs](https://arxiv.org/pdf/1507.00723v3)** | 2015-12-08 | <details><summary>Show</summary><p>A general theory of programs, programming and programming languages built up from a few concepts of elementary set theory. Derives, as theorems, properties treated as axioms by classic approaches to programming. Covers sequential and concurrent computation.</p></details> |  |
| **[Program Repair](https://arxiv.org/pdf/2211.12787v1)** | 2022-11-24 | <details><summary>Show</summary><p>Automated program repair is an emerging technology which consists of a suite of techniques to automatically fix bugs or vulnerabilities in programs. In this paper, we present a comprehensive survey of the state of the art in program repair. We first study the different suite of techniques used including search based repair, constraint based repair and learning based repair. We then discuss one of the main challenges in program repair namely patch overfitting, by distilling a class of techniques which can alleviate patch overfitting. We then discuss classes of program repair tools, applications of program repair as well as uses of program repair in industry. We conclude the survey with a forward looking outlook on future usages of program repair, as well as research opportunities arising from work on code from large language models.</p></details> | <details><summary>arXiv...</summary><p>arXiv admin note: text overlap with arXiv:2012.06824 by other authors</p></details> |
| **[Is AI the better programming partner? Human-Human Pair Programming vs. Human-AI pAIr Programming](https://arxiv.org/pdf/2306.05153v2)** | 2023-06-12 | <details><summary>Show</summary><p>The emergence of large-language models (LLMs) that excel at code generation and commercial products such as GitHub's Copilot has sparked interest in human-AI pair programming (referred to as "pAIr programming") where an AI system collaborates with a human programmer. While traditional pair programming between humans has been extensively studied, it remains uncertain whether its findings can be applied to human-AI pair programming. We compare human-human and human-AI pair programming, exploring their similarities and differences in interaction, measures, benefits, and challenges. We find that the effectiveness of both approaches is mixed in the literature (though the measures used for pAIr programming are not as comprehensive). We summarize moderating factors on the success of human-human pair programming, which provides opportunities for pAIr programming research. For example, mismatched expertise makes pair programming less productive, therefore well-designed AI programming assistants may adapt to differences in expertise levels.</p></details> | <details><summary>8 pag...</summary><p>8 pages (without references), 2 tables</p></details> |
| **[Using Program Synthesis for Program Analysis](https://arxiv.org/pdf/1508.07829v1)** | 2015-09-01 | <details><summary>Show</summary><p>In this paper, we identify a fragment of second-order logic with restricted quantification that is expressive enough to capture numerous static analysis problems (e.g. safety proving, bug finding, termination and non-termination proving, superoptimisation). We call this fragment the {\it synthesis fragment}. Satisfiability of a formula in the synthesis fragment is decidable over finite domains; specifically the decision problem is NEXPTIME-complete. If a formula in this fragment is satisfiable, a solution consists of a satisfying assignment from the second order variables to \emph{functions over finite domains}. To concretely find these solutions, we synthesise \emph{programs} that compute the functions. Our program synthesis algorithm is complete for finite state programs, i.e. every \emph{function} over finite domains is computed by some \emph{program} that we can synthesise. We can therefore use our synthesiser as a decision procedure for the synthesis fragment of second-order logic, which in turn allows us to use it as a powerful backend for many program analysis tasks. To show the tractability of our approach, we evaluate the program synthesiser on several static analysis problems.</p></details> | <details><summary>19 pa...</summary><p>19 pages, to appear in LPAR 2015. arXiv admin note: text overlap with arXiv:1409.4925</p></details> |
| **[An Abstract Programming System](https://arxiv.org/pdf/cs/0306028v1)** | 2005-09-17 | <details><summary>Show</summary><p>The system PL permits the translation of abstract proofs of program correctness into programs in a variety of programming languages. A programming language satisfying certain axioms may be the target of such a translation. The system PL also permits the construction and proof of correctness of programs in an abstract programming language, and permits the translation of these programs into correct programs in a variety of languages. The abstract programming language has an imperative style of programming with assignment statements and side-effects, to allow the efficient generation of code. The abstract programs may be written by humans and then translated, avoiding the need to write the same program repeatedly in different languages or even the same language. This system uses classical logic, is conceptually simple, and permits reasoning about nonterminating programs using Scott-Strachey style denotational semantics.</p></details> | Internal report |
| **[Imperative Program Synthesis from Answer Set Programs](https://arxiv.org/pdf/1909.09058v1)** | 2019-09-20 | <details><summary>Show</summary><p>Our research concerns generating imperative programs from Answer Set Programming Specifications. ASP is highly declarative and is ideal for writing specifications. Further with negation-as-failure it is easy to succinctly represent combinatorial search problems. We are currently working on synthesizing imperative programs from ASP programs by turning the negation into useful computations. This opens up a novel way to synthesize programs from executable specifications.</p></details> | <details><summary>In Pr...</summary><p>In Proceedings ICLP 2019, arXiv:1909.07646</p></details> |
| **[Composing Programs in a Rewriting Logic for Declarative Programming](https://arxiv.org/pdf/cs/0203006v1)** | 2005-09-17 | <details><summary>Show</summary><p>Constructor-Based Conditional Rewriting Logic is a general framework for integrating first-order functional and logic programming which gives an algebraic semantics for non-deterministic functional-logic programs. In the context of this formalism, we introduce a simple notion of program module as an open program which can be extended together with several mechanisms to combine them. These mechanisms are based on a reduced set of operations. However, the high expressiveness of these operations enable us to model typical constructs for program modularization like hiding, export/import, genericity/instantiation, and inheritance in a simple way. We also deal with the semantic aspects of the proposal by introducing an immediate consequence operator, and studying several alternative semantics for a program module, based on this operator, in the line of logic programming: the operator itself, its least fixpoint (the least model of the module), the set of its pre-fixpoints (term models of the module), and some other variations in order to find a compositional and fully abstract semantics wrt the set of operations and a natural notion of observability.</p></details> | <details><summary>47 pa...</summary><p>47 pages. A shorter version (33 pages) will appear in the Journal of Theory and Practice of Logic Programming</p></details> |
| **[Programs as Polypeptides](https://arxiv.org/pdf/1506.01573v1)** | 2015-06-05 | <details><summary>Show</summary><p>We describe a visual programming language for defining behaviors manifested by reified actors in a 2D virtual world that can be compiled into programs comprised of sequences of combinators that are themselves reified as actors. This makes it possible to build programs that build programs from components of a few fixed types delivered by diffusion using processes that resemble chemistry as much as computation.</p></details> | <details><summary>in Eu...</summary><p>in European Conference on Artificial Life (ECAL '15), York, UK, 2015</p></details> |
| **[Disjunctive Logic Programs versus Normal Logic Programs](https://arxiv.org/pdf/1304.0620v1)** | 2013-04-03 | <details><summary>Show</summary><p>This paper focuses on the expressive power of disjunctive and normal logic programs under the stable model semantics over finite, infinite, or arbitrary structures. A translation from disjunctive logic programs into normal logic programs is proposed and then proved to be sound over infinite structures. The equivalence of expressive power of two kinds of logic programs over arbitrary structures is shown to coincide with that over finite structures, and coincide with whether or not NP is closed under complement. Over finite structures, the intranslatability from disjunctive logic programs to normal logic programs is also proved if arities of auxiliary predicates and functions are bounded in a certain way.</p></details> |  |
| **[Weighted Programming](https://arxiv.org/pdf/2202.07577v2)** | 2022-04-01 | <details><summary>Show</summary><p>We study weighted programming, a programming paradigm for specifying mathematical models. More specifically, the weighted programs we investigate are like usual imperative programs with two additional features: (1) nondeterministic branching and (2) weighting execution traces. Weights can be numbers but also other objects like words from an alphabet, polynomials, formal power series, or cardinal numbers. We argue that weighted programming as a paradigm can be used to specify mathematical models beyond probability distributions (as is done in probabilistic programming). We develop weakest-precondition- and weakest-liberal-precondition-style calculi à la Dijkstra for reasoning about mathematical models specified by weighted programs. We present several case studies. For instance, we use weighted programming to model the ski rental problem - an optimization problem. We model not only the optimization problem itself, but also the best deterministic online algorithm for solving this problem as weighted programs. By means of weakest-precondition-style reasoning, we can determine the competitive ratio of the online algorithm on source code level.</p></details> | 71 pages |
| **[Choose Your Programming Copilot: A Comparison of the Program Synthesis Performance of GitHub Copilot and Genetic Programming](https://arxiv.org/pdf/2111.07875v1)** | 2021-11-16 | <details><summary>Show</summary><p>GitHub Copilot, an extension for the Visual Studio Code development environment powered by the large-scale language model Codex, makes automatic program synthesis available for software developers. This model has been extensively studied in the field of deep learning, however, a comparison to genetic programming, which is also known for its performance in automatic program synthesis, has not yet been carried out. In this paper, we evaluate GitHub Copilot on standard program synthesis benchmark problems and compare the achieved results with those from the genetic programming literature. In addition, we discuss the performance of both approaches. We find that the performance of the two approaches on the benchmark problems is quite similar, however, in comparison to GitHub Copilot, the program synthesis approaches based on genetic programming are not yet mature enough to support programmers in practical software development. Genetic programming usually needs a huge amount of expensive hand-labeled training cases and takes too much time to generate solutions. Furthermore, source code generated by genetic programming approaches is often bloated and difficult to understand. For future work on program synthesis with genetic programming, we suggest researchers to focus on improving the execution time, readability, and usability.</p></details> |  |
| **[Programming Puzzles](https://arxiv.org/pdf/2106.05784v3)** | 2021-11-09 | <details><summary>Show</summary><p>We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program $f$, and the goal is to find an input which makes $f$ return True. The puzzles are objective in that each one is specified entirely by the source code of its verifier $f$, so evaluating $f$ is all that is needed to test a candidate solution. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems, to classic programming puzzles (e.g., Tower of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). We develop baseline enumerative program synthesis, GPT-3 and Codex solvers that are capable of solving puzzles -- even without access to any reference solutions -- by learning from their own past solutions. Codex performs best, solving up to 18% of 397 test problems with a single try and 80% of the problems with 1,000 tries per problem. In a small user study, we find a positive correlation between puzzle-solving performance and coding experience, and between the puzzle difficulty for humans and AI solvers. Therefore, further improvements on P3 could have a significant impact on many program synthesis areas.</p></details> | <details><summary>NeurI...</summary><p>NeurIPS 2021 (Datasets and Benchmarks Track). Puzzles repository: https://github.com/microsoft/PythonProgrammingPuzzles</p></details> |
| **[Quasiconvex Programming](https://arxiv.org/pdf/cs/0412046v1)** | 2005-09-17 | <details><summary>Show</summary><p>We define quasiconvex programming, a form of generalized linear programming in which one seeks the point minimizing the pointwise maximum of a collection of quasiconvex functions. We survey algorithms for solving quasiconvex programs either numerically or via generalizations of the dual simplex method from linear programming, and describe varied applications of this geometric optimization technique in meshing, scientific computation, information visualization, automated algorithm analysis, and robust statistics.</p></details> | 33 pages, 14 figures |
| **[Computer-Assisted Program Reasoning Based on a Relational Semantics of Programs](https://arxiv.org/pdf/1202.4834v1)** | 2012-02-23 | <details><summary>Show</summary><p>We present an approach to program reasoning which inserts between a program and its verification conditions an additional layer, the denotation of the program expressed in a declarative form. The program is first translated into its denotation from which subsequently the verification conditions are generated. However, even before (and independently of) any verification attempt, one may investigate the denotation itself to get insight into the "semantic essence" of the program, in particular to see whether the denotation indeed gives reason to believe that the program has the expected behavior. Errors in the program and in the meta-information may thus be detected and fixed prior to actually performing the formal verification. More concretely, following the relational approach to program semantics, we model the effect of a program as a binary relation on program states. A formal calculus is devised to derive from a program a logic formula that describes this relation and is subject for inspection and manipulation. We have implemented this idea in a comprehensive form in the RISC ProgramExplorer, a new program reasoning environment for educational purposes which encompasses the previously developed RISC ProofNavigator as an interactive proving assistant.</p></details> | <details><summary>In Pr...</summary><p>In Proceedings THedu'11, arXiv:1202.4535</p></details> |
| **[CPBVP: A Constraint-Programming Framework for Bounded Program Verification](https://arxiv.org/pdf/0807.2383v1)** | 2008-07-16 | <details><summary>Show</summary><p>This paper studies how to verify the conformity of a program with its specification and proposes a novel constraint-programming framework for bounded program verification (CPBPV). The CPBPV framework uses constraint stores to represent the specification and the program and explores execution paths nondeterministically. The input program is partially correct if each constraint store so produced implies the post-condition. CPBPV does not explore spurious execution paths as it incrementally prunes execution paths early by detecting that the constraint store is not consistent. CPBPV uses the rich language of constraint programming to express the constraint store. Finally, CPBPV is parametrized with a list of solvers which are tried in sequence, starting with the least expensive and less general. Experimental results often produce orders of magnitude improvements over earlier approaches, running times being often independent of the variable domains. Moreover, CPBPV was able to detect subtle errors in some programs while other frameworks based on model checking have failed.</p></details> |  |
| **[Programming errors in traversal programs over structured data](https://arxiv.org/pdf/1201.6057v1)** | 2012-01-31 | <details><summary>Show</summary><p>Traversal strategies á la Stratego (also á la Strafunski and 'Scrap Your Boilerplate') provide an exceptionally versatile and uniform means of querying and transforming deeply nested and heterogeneously structured data including terms in functional programming and rewriting, objects in OO programming, and XML documents in XML programming. However, the resulting traversal programs are prone to programming errors. We are specifically concerned with errors that go beyond conservative type errors; examples we examine include divergent traversals, prematurely terminated traversals, and traversals with dead code. Based on an inventory of possible programming errors we explore options of static typing and static analysis so that some categories of errors can be avoided. This exploration generates suggestions for improvements to strategy libraries as well as their underlying programming languages. Haskell is used for illustrations and specifications with sufficient explanations to make the presentation comprehensible to the non-specialist. The overall ideas are language-agnostic and they are summarized accordingly.</p></details> |  |
| **[Quantum Predicative Programming](https://arxiv.org/pdf/quant-ph/0602156v1)** | 2008-02-19 | <details><summary>Show</summary><p>The subject of this work is quantum predicative programming -- the study of developing of programs intended for execution on a quantum computer. We look at programming in the context of formal methods of program development, or programming methodology. Our work is based on probabilistic predicative programming, a recent generalisation of the well-established predicative programming. It supports the style of program development in which each programming step is proven correct as it is made. We inherit the advantages of the theory, such as its generality, simple treatment of recursive programs, time and space complexity, and communication. Our theory of quantum programming provides tools to write both classical and quantum specifications, develop quantum programs that implement these specifications, and reason about their comparative time and space complexity all in the same framework.</p></details> |  |
| **[Program structure](https://arxiv.org/pdf/0907.5290v2)** | 2012-03-23 | <details><summary>Show</summary><p>A program is usually represented as a word chain. It is exactly a word chain that appears as the lexical analyzer output and is parsed. The work shows that a program can be syntactically represented as an oriented word tree, that is a syntactic program tree, program words being located both in tree nodes and on tree arrows. The basic property of a tree is that arrows starting from each node are marked by different words (including an empty word). Semantics can then be directly specified on such tree using either requirements or additional links, and adding instructions to some tree nodes enables program execution specification.</p></details> | 22 pages, 4 figures |
| **[Variational Program Inference](https://arxiv.org/pdf/1006.0991v1)** | 2010-06-08 | <details><summary>Show</summary><p>We introduce a framework for representing a variety of interesting problems as inference over the execution of probabilistic model programs. We represent a "solution" to such a problem as a guide program which runs alongside the model program and influences the model program's random choices, leading the model program to sample from a different distribution than from its priors. Ideally the guide program influences the model program to sample from the posteriors given the evidence. We show how the KL- divergence between the true posterior distribution and the distribution induced by the guided model program can be efficiently estimated (up to an additive constant) by sampling multiple executions of the guided model program. In addition, we show how to use the guide program as a proposal distribution in importance sampling to statistically prove lower bounds on the probability of the evidence and on the probability of a hypothesis and the evidence. We can use the quotient of these two bounds as an estimate of the conditional probability of the hypothesis given the evidence. We thus turn the inference problem into a heuristic search for better guide programs.</p></details> |  |
| **[Probabilistic Program Abstractions](https://arxiv.org/pdf/1705.09970v2)** | 2017-07-17 | <details><summary>Show</summary><p>Abstraction is a fundamental tool for reasoning about complex systems. Program abstraction has been utilized to great effect for analyzing deterministic programs. At the heart of program abstraction is the relationship between a concrete program, which is difficult to analyze, and an abstract program, which is more tractable. Program abstractions, however, are typically not probabilistic. We generalize non-deterministic program abstractions to probabilistic program abstractions by explicitly quantifying the non-deterministic choices. Our framework upgrades key definitions and properties of abstractions to the probabilistic context. We also discuss preliminary ideas for performing inference on probabilistic abstractions and general probabilistic programs.</p></details> |  |
| **[Dynamic Programming and Linear Programming for Odds Problem](https://arxiv.org/pdf/2107.13146v1)** | 2021-07-29 | <details><summary>Show</summary><p>This paper discusses the odds problem, proposed by Bruss in 2000, and its variants. A recurrence relation called a dynamic programming (DP) equation is used to find an optimal stopping policy of the odds problem and its variants. In 2013, Buchbinder, Jain, and Singh proposed a linear programming (LP) formulation for finding an optimal stopping policy of the classical secretary problem, which is a special case of the odds problem. The proposed linear programming problem, which maximizes the probability of a win, differs from the DP equations known for long time periods. This paper shows that an ordinary DP equation is a modification of the dual problem of linear programming including the LP formulation proposed by Buchbinder, Jain, and Singh.</p></details> | 12 pages, 1 figure |
| **[Alternation in Quantum Programming: From Superposition of Data to Superposition of Programs](https://arxiv.org/pdf/1402.5172v1)** | 2014-02-24 | <details><summary>Show</summary><p>We extract a novel quantum programming paradigm - superposition of programs - from the design idea of a popular class of quantum algorithms, namely quantum walk-based algorithms. The generality of this paradigm is guaranteed by the universality of quantum walks as a computational model. A new quantum programming language QGCL is then proposed to support the paradigm of superposition of programs. This language can be seen as a quantum extension of Dijkstra's GCL (Guarded Command Language). Surprisingly, alternation in GCL splits into two different notions in the quantum setting: classical alternation (of quantum programs) and quantum alternation, with the latter being introduced in QGCL for the first time. Quantum alternation is the key program construct for realizing the paradigm of superposition of programs. The denotational semantics of QGCL are defined by introducing a new mathematical tool called the guarded composition of operator-valued functions. Then the weakest precondition semantics of QGCL can straightforwardly derived. Another very useful program construct in realizing the quantum programming paradigm of superposition of programs, called quantum choice, can be easily defined in terms of quantum alternation. The relation between quantum choices and probabilistic choices is clarified through defining the notion of local variables. We derive a family of algebraic laws for QGCL programs that can be used in program verification, transformations and compilation. The expressive power of QGCL is illustrated by several examples where various variants and generalizations of quantum walks are conveniently expressed using quantum alternation and quantum choice. We believe that quantum programming with quantum alternation and choice will play an important role in further exploiting the power of quantum computing.</p></details> | <details><summary>arXiv...</summary><p>arXiv admin note: substantial text overlap with arXiv:1209.4379</p></details> |
| **[The Elements of Differentiable Programming](https://arxiv.org/pdf/2403.14606v3)** | 2025-06-25 | <details><summary>Show</summary><p>Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.</p></details> | Draft version 3 |
| **[Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces](https://arxiv.org/pdf/2505.17703v1)** | 2025-05-26 | <details><summary>Show</summary><p>Automatic program repair seeks to generate correct code from buggy programs, with most approaches searching the correct program in a discrete, symbolic space of source code tokens. This symbolic search is fundamentally limited by its inability to directly reason about program behavior. We introduce Gradient-Based Program Repair (GBPR), a new paradigm that reframes program repair as continuous optimization in a differentiable numerical program space. Our core insight is to compile symbolic programs into differentiable numerical representations, enabling search in the numerical program space directly guided by program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of 1,466 buggy symbolic RASP programs and their respective numerical representations. Our experiments demonstrate that GBPR can effectively repair buggy symbolic programs by gradient-based optimization in the numerical program space, with convincing repair trajectories. To our knowledge, we are the first to state program repair as continuous optimization in a numerical program space. Our work establishes a new direction for program repair research, bridging two rich worlds: continuous optimization and program behavior.</p></details> |  |
| **[Programming in Alma-0, or Imperative and Declarative Programming Reconciled](https://arxiv.org/pdf/cs/0004002v1)** | 2005-09-17 | <details><summary>Show</summary><p>In (Apt et al, TOPLAS 1998) we introduced the imperative programming language Alma-0 that supports declarative programming. In this paper we illustrate the hybrid programming style of Alma-0 by means of various examples that complement those presented in (Apt et al, TOPLAS 1998). The presented Alma-0 programs illustrate the versatility of the language and show that ``don't know'' nondeterminism can be naturally combined with assignment.</p></details> | <details><summary>With ...</summary><p>With updated references with respect to the published version</p></details> |
| **[Generating Random Logic Programs Using Constraint Programming](https://arxiv.org/pdf/2006.01889v2)** | 2020-09-14 | <details><summary>Show</summary><p>Testing algorithms across a wide range of problem instances is crucial to ensure the validity of any claim about one algorithm's superiority over another. However, when it comes to inference algorithms for probabilistic logic programs, experimental evaluations are limited to only a few programs. Existing methods to generate random logic programs are limited to propositional programs and often impose stringent syntactic restrictions. We present a novel approach to generating random logic programs and random probabilistic logic programs using constraint programming, introducing a new constraint to control the independence structure of the underlying probability distribution. We also provide a combinatorial argument for the correctness of the model, show how the model scales with parameter values, and use the model to compare probabilistic inference algorithms across a range of synthetic problems. Our model allows inference algorithm developers to evaluate and compare the algorithms across a wide range of instances, providing a detailed picture of their (comparative) strengths and weaknesses.</p></details> | <details><summary>This ...</summary><p>This is an extended version of the paper published in CP 2020</p></details> |
| **[Semantics of Sets of Programs](https://arxiv.org/pdf/2410.16102v2)** | 2025-07-29 | <details><summary>Show</summary><p>Applications like program synthesis sometimes require proving that a property holds for all of the infinitely many programs described by a grammar - i.e., an inductively defined set of programs. Current verification frameworks overapproximate programs' behavior when sets of programs contain loops, including two Hoare-style logics that fail to be relatively complete when loops are allowed. In this work, we prove that compositionally verifying simple properties for infinite sets of programs requires tracking distinct program behaviors over unboundedly many executions. Tracking this information is both necessary and sufficient for verification. We prove this fact in a general, reusable theory of denotational semantics that can model the expressivity and compositionality of verification techniques over infinite sets of programs. We construct the minimal compositional semantics that captures simple properties of sets of programs and use it to derive the first sound and relatively complete Hoare-style logic for infinite sets of programs. Thus, our methods can be used to design minimally complex, compositional verification techniques for sets of programs.</p></details> | 47 pages, 8 Figures |
| **[Accurate Programming: Thinking about programs in terms of properties](https://arxiv.org/pdf/1109.0786v1)** | 2011-09-06 | <details><summary>Show</summary><p>Accurate programming is a practical approach to producing high quality programs. It combines ideas from test-automation, test-driven development, agile programming, and other state of the art software development methods. In addition to building on approaches that have proven effective in practice, it emphasizes concepts that help programmers sharpen their understanding of both the problems they are solving and the solutions they come up with. This is achieved by encouraging programmers to think about programs in terms of properties.</p></details> | <details><summary>In Pr...</summary><p>In Proceedings DSL 2011, arXiv:1109.0323</p></details> |
| **[Moving beyond Deletions: Program Simplification via Diverse Program Transformations](https://arxiv.org/pdf/2401.15234v1)** | 2024-01-30 | <details><summary>Show</summary><p>To reduce the complexity of software, Developers manually simplify program (known as developer-induced program simplification in this paper) to reduce its code size yet preserving its functionality but manual simplification is time-consuming and error-prone. To reduce manual effort, rule-based approaches (e.g., refactoring) and deletion-based approaches (e.g., delta debugging) can be potentially applied to automate developer-induced program simplification. However, as there is little study on how developers simplify programs in Open-source Software (OSS) projects, it is unclear whether these approaches can be effectively used for developer-induced program simplification. Hence, we present the first study of developer-induced program simplification in OSS projects, focusing on the types of program transformations used, the motivations behind simplifications, and the set of program transformations covered by existing refactoring types. Our study of 382 pull requests from 296 projects reveals that there exist gaps in applying existing approaches for automating developer-induced program simplification. and outlines the criteria for designing automatic program simplification techniques. Inspired by our study and to reduce the manual effort in developer-induced program simplification, we propose SimpT5, a tool that can automatically produce simplified programs (semantically-equivalent programs with reduced source lines of code). SimpT5 is trained based on our collected dataset of 92,485 simplified programs with two heuristics: (1) simplified line localization that encodes lines changed in simplified programs, and (2)checkers that measure the quality of generated programs. Our evaluation shows that SimpT5 are more effective than prior approaches in automating developer-induced program simplification.</p></details> |  |
| **[The Impact of Program Reduction on Automated Program Repair](https://arxiv.org/pdf/2408.01134v1)** | 2024-08-05 | <details><summary>Show</summary><p>Correcting bugs using modern Automated Program Repair (APR) can be both time-consuming and resource-expensive. We describe a program repair approach that aims to improve the scalability of modern APR tools. The approach leverages program reduction in the form of program slicing to eliminate code irrelevant to fixing the bug, which improves the APR tool's overall performance. We investigate slicing's impact on all three phases of the repair process: fault localization, patch generation, and patch validation. Our empirical exploration finds that the proposed approach, on average, enhances the repair ability of the TBar APR tool, but we also discovered a few cases where it was less successful. Specifically, on examples from the widely used Defects4J dataset, we obtain a substantial reduction in median repair time, which falls from 80 minutes to just under 18 minutes. We conclude that program reduction can improve the performance of APR without degrading repair quality, but this improvement is not universal. A replication package is available via Zenodo at https://doi.org/10.5281/zenodo.13074333. Keywords: automated program repair, dynamic program slicing, fault localization, test-suite reduction, hybrid techniques.</p></details> | <details><summary>Accep...</summary><p>Accepted for publication as full research paper in the 40th IEEE International Conference on Software Maintenance and Evolution (ICSME 2024)</p></details> |
| **[Verification of Imperative Programs by Constraint Logic Program Transformation](https://arxiv.org/pdf/1309.5139v1)** | 2013-09-23 | <details><summary>Show</summary><p>We present a method for verifying partial correctness properties of imperative programs that manipulate integers and arrays by using techniques based on the transformation of constraint logic programs (CLP). We use CLP as a metalanguage for representing imperative programs, their executions, and their properties. First, we encode the correctness of an imperative program, say prog, as the negation of a predicate 'incorrect' defined by a CLP program T. By construction, 'incorrect' holds in the least model of T if and only if the execution of prog from an initial configuration eventually halts in an error configuration. Then, we apply to program T a sequence of transformations that preserve its least model semantics. These transformations are based on well-known transformation rules, such as unfolding and folding, guided by suitable transformation strategies, such as specialization and generalization. The objective of the transformations is to derive a new CLP program TransfT where the predicate 'incorrect' is defined either by (i) the fact 'incorrect.' (and in this case prog is not correct), or by (ii) the empty set of clauses (and in this case prog is correct). In the case where we derive a CLP program such that neither (i) nor (ii) holds, we iterate the transformation. Since the problem is undecidable, this process may not terminate. We show through examples that our method can be applied in a rather systematic way, and is amenable to automation by transferring to the field of program verification many techniques developed in the field of program transformation.</p></details> | <details><summary>In Pr...</summary><p>In Proceedings Festschrift for Dave Schmidt, arXiv:1309.4557</p></details> |
| **[On the Verification of Belief Programs](https://arxiv.org/pdf/2204.12562v3)** | 2022-05-04 | <details><summary>Show</summary><p>In a recent paper, Belle and Levesque proposed a framework for a type of program called belief programs, a probabilistic extension of GOLOG programs where every action and sensing result could be noisy and every test condition refers to the agent's subjective beliefs. Inherited from GOLOG programs, the action-centered feature makes belief programs fairly suitable for high-level robot control under uncertainty. An important step before deploying such a program is to verify whether it satisfies properties as desired. At least two problems exist in doing verification: how to formally specify properties of a program and what is the complexity of verification. In this paper, we propose a formalism for belief programs based on a modal logic of actions and beliefs. Among other things, this allows us to express PCTL-like temporal properties smoothly. Besides, we investigate the decidability and undecidability for the verification problem of belief programs.</p></details> | unpublished |
| **[Impact of Indentation in Programming](https://arxiv.org/pdf/1311.3130v1)** | 2013-11-14 | <details><summary>Show</summary><p>In computer programming languages, indentation formats program source code to improve readability. Programming languages make use of indentation to define program structure .Programmers use indentation to understand the structure of their programs to human readers. Especially, indentation is the better way to represent the relationship between control flow constructs such as selection statements or loops and code contained within and outside them. This paper describes about different indentation styles used in Programming and also describes context of each indentation style. It also describes indentation styles used for various programming constructs and the best practice for a particular programming construct. This paper helps the beginners to understand various indentation styles used in programming and also to choose suitable indentation style.</p></details> |  |
| **[Concept of the abstract program](https://arxiv.org/pdf/1207.6369v1)** | 2016-11-26 | <details><summary>Show</summary><p>The aim of this paper is to alter the abstract definition of the program of the theoretical programming model which has been developed at Eotvos Lorand University for many years in order to investigate methods that support designing correct programs. The motivation of this modification was that the dynamic properties of programs appear in the model. This new definition of the program gives a hand to extend the model with the concept of subprograms while the earlier results of the original programming model are preserved.</p></details> |  |
| **[An Introduction to Probabilistic Programming](https://arxiv.org/pdf/1809.10756v2)** | 2021-10-20 | <details><summary>Show</summary><p>This book is a graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning is a foundational computation central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a first-order probabilistic programming language (PPL) whose programs correspond to graphical models with a known, finite, set of random variables. In the context of this PPL we introduce fundamental inference algorithms and describe how they can be implemented. We then turn to higher-order probabilistic programming languages. Programs in such languages can define models with dynamic computation graphs, which may not instantiate the same set of random variables in each execution. Inference requires methods that generate samples by repeatedly evaluating the program. Foundational algorithms for this kind of language are discussed in the context of an interface between program executions and an inference controller. Finally we consider the intersection of probabilistic and differentiable programming. We begin with a discussion of automatic differentiation, and how it can be used to implement efficient inference methods based on Hamiltonian Monte Carlo. We then discuss gradient-based maximum likelihood estimation in programs that are parameterized using neural networks, how to amortize inference using by learning neural approximations to the program posterior, and how language features impact the design of deep probabilistic programming systems.</p></details> | <details><summary>Under...</summary><p>Under review at Foundations and Trends in Machine Learning</p></details> |
| **[A new method for reducing algebraic programs to polynomial programs](https://arxiv.org/pdf/2502.08210v1)** | 2025-02-13 | <details><summary>Show</summary><p>We consider a generalization of polynomial programs: algebraic programs, which are optimization or feasibility problems with algebraic objectives or constraints. Algebraic functions are defined as zeros of multivariate polynomials. They are a rich set of functions that includes polynomials themselves, but also ratios and radicals, and finite compositions thereof. When an algebraic program is given in terms of radical expressions, a straightforward way of reformulating into a polynomial program is to introduce a new variable for each distinct radical that appears. Hence, the rich theory and algorithms for polynomial programs, including satisfiability via cylindrical algebraic decomposition, infeasibility certificates via Positivstellensatz theorems, and optimization with sum-of-squares programming directly apply to algebraic programs. We propose a different reformulation, that in many cases introduces significantly fewer new variables, and thus produces polynomial programs that are easier to solve. First, we exhibit an algorithm that finds a defining polynomial of an algebraic function given as a radical expression. As a polynomial does not in general define a unique algebraic function, additional constraints need to be added that isolate the algebraic function from others defined by the same polynomial. Using results from real algebraic geometry, we develop an algorithm that generates polynomial inequalities that isolate an algebraic function. This allows us to reformulate an algebraic program into a polynomial one, by introducing only a single new variable for each algebraic function. On modified versions of classic optimization benchmarks with added algebraic terms, our formulation achieves speedups of up to 50x compared to the straightforward reformulation.</p></details> | <details><summary>18 pa...</summary><p>18 pages, 2 figures, 2 tables</p></details> |
| **[T2Script Programming Language](https://arxiv.org/pdf/1101.5569v1)** | 2015-03-18 | <details><summary>Show</summary><p>Event-driven programming is used in many fields of modern Computer Science. In event-driven programming languages user interacts with a program by triggering the events. We propose a new approach that we denote command-event driven programming in which the user interacts with a program by means of events and commands. We describe a new programming language, T2Script, which is based on command-event driven paradigm. T2Script has been already implemented and used in one of industrial products. We describe the rationale, basic concepts and advanced programming techniques of new T2Script language. We evaluate the new language and show what advantages and limitations it has.</p></details> | 27 pages, 9 figures |
| **[Logic programming in the context of multiparadigm programming: the Oz experience](https://arxiv.org/pdf/cs/0208029v1)** | 2005-09-17 | <details><summary>Show</summary><p>Oz is a multiparadigm language that supports logic programming as one of its major paradigms. A multiparadigm language is designed to support different programming paradigms (logic, functional, constraint, object-oriented, sequential, concurrent, etc.) with equal ease. This article has two goals: to give a tutorial of logic programming in Oz and to show how logic programming fits naturally into the wider context of multiparadigm programming. Our experience shows that there are two classes of problems, which we call algorithmic and search problems, for which logic programming can help formulate practical solutions. Algorithmic problems have known efficient algorithms. Search problems do not have known efficient algorithms but can be solved with search. The Oz support for logic programming targets these two problem classes specifically, using the concepts needed for each. This is in contrast to the Prolog approach, which targets both classes with one set of concepts, which results in less than optimal support for each class. To explain the essential difference between algorithmic and search programs, we define the Oz execution model. This model subsumes both concurrent logic programming (committed-choice-style) and search-based logic programming (Prolog-style). Instead of Horn clause syntax, Oz has a simple, fully compositional, higher-order syntax that accommodates the abilities of the language. We conclude with lessons learned from this work, a brief history of Oz, and many entry points into the Oz literature.</p></details> | <details><summary>48 pa...</summary><p>48 pages, to appear in the journal "Theory and Practice of Logic Programming"</p></details> |
| **[Summary - TerpreT: A Probabilistic Programming Language for Program Induction](https://arxiv.org/pdf/1612.00817v1)** | 2016-12-05 | <details><summary>Show</summary><p>We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TerpreT, a domain-specific language for expressing program synthesis problems. A TerpreT model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inference task is to observe a set of input-output examples and infer the underlying program. From a TerpreT model we automatically perform inference using four different back-ends: gradient descent (thus each TerpreT model can be seen as defining a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper comparisons between different approaches to inference. We illustrate the value of TerpreT by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models. To our knowledge, this is the first work to compare gradient-based search over program space to traditional search-based alternatives. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations. This is a workshop summary of a longer report at arXiv:1608.04428</p></details> | <details><summary>7 pag...</summary><p>7 pages, 2 figures, 4 tables in 1st Workshop on Neural Abstract Machines & Program Induction (NAMPI), @NIPS 2016</p></details> |
| **[Context-Oriented Programming: A Programming Paradigm for Autonomic Systems](https://arxiv.org/pdf/1105.0069v2)** | 2012-04-02 | <details><summary>Show</summary><p>Dynamic software adaptability is one of the central features leveraged by autonomic computing. However, developing software that changes its behavior at run time adapting to the operational conditions is a challenging task. Several approaches have been proposed in the literature to attack this problem at different and complementary abstraction levels: software architecture, middleware, and programming level. We focus on the support that ad-hoc programming language constructs may provide to support dynamically adaptive behaviors. We introduce context-oriented programming languages and we present a framework that positions the supported paradigm in the MAPE-K autonomic loop. We discuss the advantages of using context-oriented programming languages instead of other mainstream approaches based on dynamic aspect oriented programming languages and present a case study that shows how the proposed programming style naturally fits dynamic adaptation requirements. Finally, we discuss some known problems and outline a number of open research challenges.</p></details> |  |
| **[The First Computer Program](https://arxiv.org/pdf/2303.13740v1)** | 2023-03-27 | <details><summary>Show</summary><p>In 1837, the first computer program in history was sketched by the renowned mathematician and inventor Charles Babbage. It was a program for the Analytical Engine. The program consists of a sequence of arithmetical operations and the necessary variable addresses (memory locations) of the arguments and the result, displayed in tabular fashion, like a program trace. The program computes the solutions for a system of two linear equations in two unknowns.</p></details> | 8 pages, 4 tables |
| **[Explanations as Programs in Probabilistic Logic Programming](https://arxiv.org/pdf/2210.03021v2)** | 2023-08-17 | <details><summary>Show</summary><p>The generation of comprehensible explanations is an essential feature of modern artificial intelligence systems. In this work, we consider probabilistic logic programming, an extension of logic programming which can be useful to model domains with relational structure and uncertainty. Essentially, a program specifies a probability distribution over possible worlds (i.e., sets of facts). The notion of explanation is typically associated with that of a world, so that one often looks for the most probable world as well as for the worlds where the query is true. Unfortunately, such explanations exhibit no causal structure. In particular, the chain of inferences required for a specific prediction (represented by a query) is not shown. In this paper, we propose a novel approach where explanations are represented as programs that are generated from a given query by a number of unfolding-like transformations. Here, the chain of inferences that proves a given query is made explicit. Furthermore, the generated explanations are minimal (i.e., contain no irrelevant information) and can be parameterized w.r.t. a specification of visible predicates, so that the user may hide uninteresting details from explanations.</p></details> | <details><summary>Publi...</summary><p>Published as: Vidal, G. (2022). Explanations as Programs in Probabilistic Logic Programming. In: Hanus, M., Igarashi, A. (eds) Functional and Logic Programming. FLOPS 2022. Lecture Notes in Computer Science, vol 13215. Springer, Cham. The final authenticated publication is available online at https://doi.org/10.1007/978-3-030-99461-7_12</p></details> |
| **[Stochastic Probabilistic Programs](https://arxiv.org/pdf/2001.02656v3)** | 2020-01-23 | <details><summary>Show</summary><p>We introduce the notion of a stochastic probabilistic program and present a reference implementation of a probabilistic programming facility supporting specification of stochastic probabilistic programs and inference in them. Stochastic probabilistic programs allow straightforward specification and efficient inference in models with nuisance parameters, noise, and nondeterminism. We give several examples of stochastic probabilistic programs, and compare the programs with corresponding deterministic probabilistic programs in terms of model specification and inference. We conclude with discussion of open research topics and related work.</p></details> | <details><summary>7 pag...</summary><p>7 pages main body, 4 pages appendix</p></details> |
| **[Quantifying Program Bias](https://arxiv.org/pdf/1702.05437v2)** | 2017-03-08 | <details><summary>Show</summary><p>With the range and sensitivity of algorithmic decisions expanding at a break-neck speed, it is imperative that we aggressively investigate whether programs are biased. We propose a novel probabilistic program analysis technique and apply it to quantifying bias in decision-making programs. Specifically, we (i) present a sound and complete automated verification technique for proving quantitative properties of probabilistic programs; (ii) show that certain notions of bias, recently proposed in the fairness literature, can be phrased as quantitative correctness properties; and (iii) present FairSquare, the first verification tool for quantifying program bias, and evaluate it on a range of decision-making programs.</p></details> |  |
| **[Deployable probabilistic programming](https://arxiv.org/pdf/1906.11199v1)** | 2019-06-27 | <details><summary>Show</summary><p>We propose design guidelines for a probabilistic programming facility suitable for deployment as a part of a production software system. As a reference implementation, we introduce Infergo, a probabilistic programming facility for Go, a modern programming language of choice for server-side software development. We argue that a similar probabilistic programming facility can be added to most modern general-purpose programming languages. Probabilistic programming enables automatic tuning of program parameters and algorithmic decision making through probabilistic inference based on the data. To facilitate addition of probabilistic programming capabilities to other programming languages, we share implementation choices and techniques employed in development of Infergo. We illustrate applicability of Infergo to various use cases on case studies, and evaluate Infergo's performance on several benchmarks, comparing Infergo to dedicated inference-centric probabilistic programming frameworks.</p></details> | <details><summary>15 pa...</summary><p>15 pages, to appear in SLPASH Onward! 2019</p></details> |
| **[Programs as proofs](https://arxiv.org/pdf/1509.04040v1)** | 2015-09-15 | <details><summary>Show</summary><p>The Curry-Howard correspondence is about a relationship between types and programs on the one hand and propositions and proofs on the other. The implications for programming language design and program verification is an active field of research. Transformer-like semantics of internal definitions that combine a defining computation and an application will be presented. By specialisation for a given defining computation one can derive inference rules for applications of defined operations. With semantics of that kind for every operation, each application identifies an axiom in a logic defined by the programming language, so a language can be considered a theory.</p></details> |  |
| **[On the Generalizability of Neural Program Models with respect to Semantic-Preserving Program Transformations](https://arxiv.org/pdf/2008.01566v3)** | 2022-07-12 | <details><summary>Show</summary><p>With the prevalence of publicly available source code repositories to train deep neural network models, neural program models can do well in source code analysis tasks such as predicting method names in given programs that cannot be easily done by traditional program analysis techniques. Although such neural program models have been tested on various existing datasets, the extent to which they generalize to unforeseen source code is largely unknown. Since it is very challenging to test neural program models on all unforeseen programs, in this paper, we propose to evaluate the generalizability of neural program models with respect to semantic-preserving transformations: a generalizable neural program model should perform equally well on programs that are of the same semantics but of different lexical appearances and syntactical structures. We compare the results of various neural program models for the method name prediction task on programs before and after automated semantic-preserving transformations. We use three Java datasets of different sizes and three state-of-the-art neural network models for code, namely code2vec, code2seq, and GGNN, to build nine such neural program models for evaluation. Our results show that even with small semantically preserving changes to the programs, these neural program models often fail to generalize their performance. Our results also suggest that neural program models based on data and control dependencies in programs generalize better than neural program models based only on abstract syntax trees. On the positive side, we observe that as the size of the training dataset grows and diversifies the generalizability of correct predictions produced by the neural program models can be improved too. Our results on the generalizability of neural program models provide insights to measure their limitations and provide a stepping stone for their improvement.</p></details> | <details><summary>Infor...</summary><p>Information and Software Technology, IST Journal 2021, Elsevier. Related to arXiv:2004.07313</p></details> |
| **[Semantically Reflected Programs](https://arxiv.org/pdf/2509.03318v1)** | 2025-09-04 | <details><summary>Show</summary><p>This paper addresses the dichotomy between the formalization of structural and the formalization of behavioral knowledge by means of semantically lifted programs, which explore an intuitive connection between programs and knowledge graphs. While knowledge graphs and ontologies are eminently useful to represent formal knowledge about a system's individuals and universals, programming languages are designed to describe the system's evolution. To address this dichotomy, we introduce a semantic lifting of the program states of an executing program into a knowledge graph, for an object-oriented programming language. The resulting graph is exposed as a semantic reflection layer within the programming language, allowing programmers to leverage knowledge of the application domain in their programs. In this paper, we formalize semantic lifting and semantic reflection for a small programming language, SMOL, explain the operational aspects of the language, and consider type correctness and virtualisation for runtime program queries through the semantic reflection layer. We illustrate semantic lifting and semantic reflection through a case study of geological modelling and discuss different applications of the technique. The language implementation is open source and available online.</p></details> |  |
| **[On finitely recursive programs](https://arxiv.org/pdf/0901.2850v1)** | 2009-05-25 | <details><summary>Show</summary><p>Disjunctive finitary programs are a class of logic programs admitting function symbols and hence infinite domains. They have very good computational properties, for example ground queries are decidable while in the general case the stable model semantics is highly undecidable. In this paper we prove that a larger class of programs, called finitely recursive programs, preserves most of the good properties of finitary programs under the stable model semantics, namely: (i) finitely recursive programs enjoy a compactness property; (ii) inconsistency checking and skeptical reasoning are semidecidable; (iii) skeptical resolution is complete for normal finitely recursive programs. Moreover, we show how to check inconsistency and answer skeptical queries using finite subsets of the ground program instantiation. We achieve this by extending the splitting sequence theorem by Lifschitz and Turner: We prove that if the input program P is finitely recursive, then the partial stable models determined by any smooth splitting omega-sequence converge to a stable model of P.</p></details> | <details><summary>26 pa...</summary><p>26 pages, Preliminary version in Proc. of ICLP 2007, Best paper award</p></details> |
| **[Program-to-Circuit: Exploiting GNNs for Program Representation and Circuit Translation](https://arxiv.org/pdf/2109.06265v1)** | 2021-09-15 | <details><summary>Show</summary><p>Circuit design is complicated and requires extensive domain-specific expertise. One major obstacle stuck on the way to hardware agile development is the considerably time-consuming process of accurate circuit quality evaluation. To significantly expedite the circuit evaluation during the translation from behavioral languages to circuit designs, we formulate it as a Program-to-Circuit problem, aiming to exploit the representation power of graph neural networks (GNNs) by representing C/C++ programs as graphs. The goal of this work is four-fold. First, we build a standard benchmark containing 40k C/C++ programs, each of which is translated to a circuit design with actual hardware quality metrics, aiming to facilitate the development of effective GNNs targeting this high-demand circuit design area. Second, 14 state-of-the-art GNN models are analyzed on the Program-to-Circuit problem. We identify key design challenges of this problem, which should be carefully handled but not yet solved by existing GNNs. The goal is to provide domain-specific knowledge for designing GNNs with suitable inductive biases. Third, we discuss three sets of real-world benchmarks for GNN generalization evaluation, and analyze the performance gap between standard programs and the real-case ones. The goal is to enable transfer learning from limited training data to real-world large-scale circuit design problems. Fourth, the Program-to-Circuit problem is a representative within the Program-to-X framework, a set of program-based analysis problems with various downstream tasks. The in-depth understanding of strength and weaknesses in applying GNNs on Program-to-Circuit could largely benefit the entire family of Program-to-X. Pioneering in this direction, we expect more GNN endeavors to revolutionize this high-demand Program-to-Circuit problem and to enrich the expressiveness of GNNs on programs.</p></details> |  |
| **[Logic Programming, Functional Programming, and Inductive Definitions](https://arxiv.org/pdf/cs/9301109v1)** | 2016-08-31 | <details><summary>Show</summary><p>An attempt at unifying logic and functional programming is reported. As a starting point, we take the view that "logic programs" are not about logic but constitute inductive definitions of sets and relations. A skeletal language design based on these considerations is sketched and a prototype implementation discussed.</p></details> |  |
| **[Functional Programming in Pattern-Match-Oriented Programming Style](https://arxiv.org/pdf/2002.06176v1)** | 2020-02-17 | <details><summary>Show</summary><p>Throughout the history of functional programming, recursion has emerged as a natural method for describing loops in programs. However, there does often exist a substantial cognitive distance between the recursive definition and the simplest explanation of an algorithm even for the basic list processing functions such as map, concat, or unique; when we explain these functions, we seldom use recursion explicitly as we do in functional programming. For example, map is often explained as follows: the map function takes a function and a list and returns a list of the results of applying the function to all the elements of the list. This paper advocates a new programming paradigm called pattern-match-oriented programming for filling this gap. An essential ingredient of our method is utilizing pattern matching for non-free data types. Pattern matching for non-free data types features non-linear pattern matching with backtracking and extensibility of pattern-matching algorithms. Several non-standard pattern constructs, such as not-patterns, loop patterns, and sequential patterns, are derived from this pattern-matching facility. Based on that result, this paper introduces many programming techniques that replace explicit recursions with an intuitive pattern by confining recursions inside patterns. We classify these techniques as pattern-match-oriented programming design patterns. These programming techniques allow us to redefine not only the most basic functions for list processing such as map, concat, or unique more elegantly than the traditional functional programming style, but also more practical mathematical algorithms and software such as a SAT solver, computer algebra system, and database query language that we had not been able to implement concisely.</p></details> |  |
| **[Neural Programming by Example](https://arxiv.org/pdf/1703.04990v1)** | 2017-03-16 | <details><summary>Show</summary><p>Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-to-end to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.</p></details> | <details><summary>7 pag...</summary><p>7 pages, Association for the Advancement of Artificial Intelligence (AAAI)</p></details> |
| **[On termination of meta-programs](https://arxiv.org/pdf/cs/0110035v3)** | 2005-09-17 | <details><summary>Show</summary><p>The term {\em meta-programming} refers to the ability of writing programs that have other programs as data and exploit their semantics. The aim of this paper is presenting a methodology allowing us to perform a correct termination analysis for a broad class of practical meta-interpreters, including negation and performing different tasks during the execution. It is based on combining the power of general orderings, used in proving termination of term-rewrite systems and programs, and on the well-known acceptability condition, used in proving termination of logic programs. The methodology establishes a relationship between the ordering needed to prove termination of the interpreted program and the ordering needed to prove termination of the meta-interpreter together with this interpreted program. If such a relationship is established, termination of one of those implies termination of the other one, i.e., the meta-interpreter preserves termination. Among the meta-interpreters that are analysed correctly are a proof trees constructing meta-interpreter, different kinds of tracers and reasoners. To appear without appendix in Theory and Practice of Logic Programming.</p></details> | <details><summary>To ap...</summary><p>To appear in Theory and Practice of Logic Programming (TPLP)</p></details> |
| **[Logic Programming with Satisfiability](https://arxiv.org/pdf/cs/0702072v1)** | 2010-09-03 | <details><summary>Show</summary><p>This paper presents a Prolog interface to the MiniSat satisfiability solver. Logic program- ming with satisfiability combines the strengths of the two paradigms: logic programming for encoding search problems into satisfiability on the one hand and efficient SAT solving on the other. This synergy between these two exposes a programming paradigm which we propose here as a logic programming pearl. To illustrate logic programming with SAT solving we give an example Prolog program which solves instances of Partial MAXSAT.</p></details> | <details><summary>8 pag...</summary><p>8 pages, 3 figures, 1 table</p></details> |
| **[InvAASTCluster: On Applying Invariant-Based Program Clustering to Introductory Programming Assignments](https://arxiv.org/pdf/2206.14175v3)** | 2025-05-01 | <details><summary>Show</summary><p>Due to the vast number of students enrolled in programming courses, there has been an increasing number of automated program repair techniques focused on introductory programming assignments (IPAs). Typically, such techniques use program clustering to take advantage of previous correct student implementations to repair a new incorrect submission. These repair techniques use clustering methods since analyzing all available correct submissions to repair a program is not feasible. However, conventional clustering methods rely on program representations based on features such as abstract syntax trees (ASTs), syntax, control flow, and data flow. This paper proposes InvAASTCluster, a novel approach for program clustering that uses dynamically generated program invariants to cluster semantically equivalent IPAs. InvAASTCluster's program representation uses a combination of the program's semantics, through its invariants, and its structure through its anonymized abstract syntax tree (AASTs). Invariants denote conditions that must remain true during program execution, while AASTs are ASTs devoid of variable and function names, retaining only their types. Our experiments show that the proposed program representation outperforms syntax-based representations when clustering a set of correct IPAs. Furthermore, we integrate InvAASTCluster into a state-of-the-art clustering-based program repair tool. Our results show that InvAASTCluster advances the current state-of-the-art when used by clustering-based repair tools by repairing around 13% more students' programs, in a shorter amount of time.</p></details> | <details><summary>31 pa...</summary><p>31 pages, 21 Figures, 5 Tables. Accepted for publication at the Journal of Systems and Software. GitHub repo: https://github.com/pmorvalho/InvAASTCluster</p></details> |
| **[The Logic of Logic Programming](https://arxiv.org/pdf/2304.13430v1)** | 2023-04-27 | <details><summary>Show</summary><p>Our position is that logic programming is not programming in the Horn clause sublogic of classical logic, but programming in a logic of (inductive) definitions. Thus, the similarity between prototypical Prolog programs (e.g., member, append, ...) and how inductive definitions are expressed in mathematical text, is not coincidental but essential. We argue here that this provides a natural solution to the main lingering semantic questions of Logic Programming and its extensions.</p></details> |  |
| **[Nested HEX-Programs](https://arxiv.org/pdf/1108.5626v1)** | 2011-09-01 | <details><summary>Show</summary><p>Answer-Set Programming (ASP) is an established declarative programming paradigm. However, classical ASP lacks subprogram calls as in procedural programming, and access to external computations (like remote procedure calls) in general. The feature is desired for increasing modularity and---assuming proper access in place---(meta-)reasoning over subprogram results. While HEX-programs extend classical ASP with external source access, they do not support calls of (sub-)programs upfront. We present nested HEX-programs, which extend HEX-programs to serve the desired feature, in a user-friendly manner. Notably, the answer sets of called sub-programs can be individually accessed. This is particularly useful for applications that need to reason over answer sets like belief set merging, user-defined aggregate functions, or preferences of answer sets.</p></details> | <details><summary>Proce...</summary><p>Proceedings of the 19th International Conference on Applications of Declarative Programming and Knowledge Management (INAP 2011)</p></details> |
| **[Disciplined Geometric Programming](https://arxiv.org/pdf/1812.04074v2)** | 2019-03-22 | <details><summary>Show</summary><p>We introduce log-log convex programs, which are optimization problems with positive variables that become convex when the variables, objective functions, and constraint functions are replaced with their logs, which we refer to as a log-log transformation. This class of problems generalizes traditional geometric programming and generalized geometric programming, and it includes interesting problems involving nonnegative matrices. We give examples of log-log convex functions, some well-known and some less so, and we develop an analog of disciplined convex programming, which we call disciplined geometric programming. Disciplined geometric programming is a subclass of log-log convex programming generated by a composition rule and a set of functions with known curvature under the log-log transformation. Finally, we describe an implementation of disciplined geometric programming as a reduction in CVXPY 1.0.</p></details> | <details><summary>Fix t...</summary><p>Fix typos: p2, changed "g_i log-log concave" to "g_i log-log affine"; p6, max of log-log convex functions is log-log convex, not concave</p></details> |
| **[Program Analysis (an Appetizer)](https://arxiv.org/pdf/2012.10086v1)** | 2020-12-21 | <details><summary>Show</summary><p>This book is an introduction to program analysis that is meant to be considerably more elementary than our advanced book Principles of Program Analysis (Springer, 2005). Rather than using flow charts as the model of programs, the book follows our introductory book Formal Methods an Appetizer (Springer, 2019) using program graphs as the model of programs. In our experience this makes the underlying ideas more accessible to our computer science and computer engineering students on the master course 02242: Program Analysis at The Technical University of Denmark. Here we have gradually replaced our use of the more elementary parts of Principles of Program Analysis with material from the current book.</p></details> | 208 pages |
| **[KitRobot: A multi-platform graphical programming IDE to program mini-robotic agents](https://arxiv.org/pdf/1501.01588v1)** | 2015-01-08 | <details><summary>Show</summary><p>The analysis, design and development of a graphical programming IDE for mini-robotic agents allows novice users to program robotic agents by a graphical drag and drop interface, without knowing the syntax and semantics of the intermediate programming language. Our work started with the definition of the syntax and semantics of the intermediate programming language. The major work is the definition of grammar for this language. The use of a graphical drag and drop interface for programming mini-robots offers a user-friendly interface to novice users. The user can program graphically by drag and drop program parts without having expertise of the intermediate programming language. The IDE is highly flexible as it uses xml technology to store program objects (i.e. loops, conditions) and robot objects (i.e. sensors, actuators). Use of xml technology allows making major changes and updating the interface without modifying the underlying design and programming.</p></details> | <details><summary>9 pag...</summary><p>9 pages, IISTE - Computer Engineering and Intelligent Systems, ISSN 2222-1719 (Paper) ISSN 2222-2863 (Online) Vol.5, No.3, 2014</p></details> |
| **[Full-Program Induction: Verifying Array Programs sans Loop Invariants](https://arxiv.org/pdf/2209.12456v1)** | 2022-09-27 | <details><summary>Show</summary><p>Arrays are commonly used in a variety of software to store and process data in loops. Automatically proving safety properties of such programs that manipulate arrays is challenging. We present a novel verification technique, called full-program induction, for proving (a sub-class of) quantified as well as quantifier-free properties of programs manipulating arrays of parametric size $N$. Instead of inducting over individual loops, our technique inducts over the entire program (possibly containing multiple loops) directly via the program parameter $N$. The technique performs non-trivial transformations of the given program and pre-conditions during the inductive step. The transformations assist in effectively reducing the assertion checking problem by transforming a program with multiple loops to a program which has fewer and simpler loops or is loop-free. Significantly, full-program induction does not require generation or use of loop-specific invariants. To assess the efficacy of our technique, we have developed a prototype tool called Vajra. We demonstrate the performance of Vajra vis-a-vis several state-of-the-art tools on a large set of array manipulating benchmarks from the international software verification competition (SV-COMP) and on several programs inspired by algebraic functions that perform polynomial computations.</p></details> | <details><summary>Invit...</summary><p>Invited paper in the International Journal on Software Tools for Technology Transfer (STTT), special issue TACAS 2022</p></details> |
| **[On Refactoring Quantum Programs](https://arxiv.org/pdf/2306.10517v1)** | 2023-06-21 | <details><summary>Show</summary><p>Refactoring is a crucial technique for improving the efficiency and maintainability of software by restructuring its internal design while preserving its external behavior. While classical programs have benefited from various refactoring methods, the field of quantum programming lacks dedicated refactoring techniques. The distinct properties of quantum computing, such as quantum superposition, entanglement, and the no-cloning principle, necessitate specialized refactoring techniques. This paper bridges this gap by presenting a comprehensive set of refactorings specifically designed for quantum programs. Each refactoring is carefully designed and explained to ensure the effective restructuring of quantum programs. Additionally, we highlight the importance of tool support in automating the refactoring process for quantum programs. Although our study focuses on the quantum programming language Q\#, our approach is applicable to other quantum programming languages, offering a general solution for enhancing the maintainability and efficiency of quantum software.</p></details> |  |
| **[Programming Not Only by Example](https://arxiv.org/pdf/1710.01291v1)** | 2025-04-24 | <details><summary>Show</summary><p>In recent years, there has been tremendous progress in automated synthesis techniques that are able to automatically generate code based on some intent expressed by the programmer. A major challenge for the adoption of synthesis remains in having the programmer communicate their intent. When the expressed intent is coarse-grained (for example, restriction on the expected type of an expression), the synthesizer often produces a long list of results for the programmer to choose from, shifting the heavy-lifting to the user. An alternative approach, successfully used in end-user synthesis is programming by example (PBE), where the user leverages examples to interactively and iteratively refine the intent. However, using only examples is not expressive enough for programmers, who can observe the generated program and refine the intent by directly relating to parts of the generated program. We present a novel approach to interacting with a synthesizer using a granular interaction model. Our approach employs a rich interaction model where (i) the synthesizer decorates a candidate program with debug information that assists in understanding the program and identifying good or bad parts, and (ii) the user is allowed to provide feedback not only on the expected output of a program, but also on the underlying program itself. That is, when the user identifies a program as (partially) correct or incorrect, they can also explicitly indicate the good or bad parts, to allow the synthesizer to accept or discard parts of the program instead of discarding the program as a whole. We show the value of our approach in a controlled user study. Our study shows that participants have strong preference to using granular feedback instead of examples, and are able to provide granular feedback much faster.</p></details> |  |
| **[Probabilistic Neural Programs](https://arxiv.org/pdf/1612.00712v1)** | 2016-12-05 | <details><summary>Show</summary><p>We present probabilistic neural programs, a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. Thus, a program describes both a collection of decisions as well as the neural network architecture used to make each one. We evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model.</p></details> | <details><summary>Appea...</summary><p>Appears in NAMPI workshop at NIPS 2016</p></details> |
| **[Relational Linear Programs](https://arxiv.org/pdf/1410.3125v1)** | 2014-10-14 | <details><summary>Show</summary><p>We propose relational linear programming, a simple framework for combing linear programs (LPs) and logic programs. A relational linear program (RLP) is a declarative LP template defining the objective and the constraints through the logical concepts of objects, relations, and quantified variables. This allows one to express the LP objective and constraints relationally for a varying number of individuals and relations among them without enumerating them. Together with a logical knowledge base, effectively a logical program consisting of logical facts and rules, it induces a ground LP. This ground LP is solved using lifted linear programming. That is, symmetries within the ground LP are employed to reduce its dimensionality, if possible, and the reduced program is solved using any off-the-shelf LP solver. In contrast to mainstream LP template languages like AMPL, which features a mixture of declarative and imperative programming styles, RLP's relational nature allows a more intuitive representation of optimization problems over relational domains. We illustrate this empirically by experiments on approximate inference in Markov logic networks using LP relaxations, on solving Markov decision processes, and on collective inference using LP support vector machines.</p></details> |  |
| **[Fast Inference for Probabilistic Answer Set Programs via the Residual Program](https://arxiv.org/pdf/2408.07524v1)** | 2025-01-22 | <details><summary>Show</summary><p>When we want to compute the probability of a query from a Probabilistic Answer Set Program, some parts of a program may not influence the probability of a query, but they impact on the size of the grounding. Identifying and removing them is crucial to speed up the computation. Algorithms for SLG resolution offer the possibility of returning the residual program which can be used for computing answer sets for normal programs that do have a total well-founded model. The residual program does not contain the parts of the program that do not influence the probability. In this paper, we propose to exploit the residual program for performing inference. Empirical results on graph datasets show that the approach leads to significantly faster inference.</p></details> | <details><summary>The p...</summary><p>The paper has been accepted at the ICLP2024 conference and under consideration in Theory and Practice of Logic Programming (TPLP)</p></details> |
| **[Learning Probabilistic Programs](https://arxiv.org/pdf/1407.2646v1)** | 2014-07-11 | <details><summary>Show</summary><p>We develop a technique for generalising from data in which models are samplers represented as program text. We establish encouraging empirical results that suggest that Markov chain Monte Carlo probabilistic programming inference techniques coupled with higher-order probabilistic programming languages are now sufficiently powerful to enable successful inference of this kind in nontrivial domains. We also introduce a new notion of probabilistic program compilation and show how the same machinery might be used in the future to compile probabilistic programs for efficient reusable predictive inference.</p></details> |  |
| **[On completeness of logic programs](https://arxiv.org/pdf/1411.3015v1)** | 2014-11-13 | <details><summary>Show</summary><p>Program correctness (in imperative and functional programming) splits in logic programming into correctness and completeness. Completeness means that a program produces all the answers required by its specification. Little work has been devoted to reasoning about completeness. This paper presents a few sufficient conditions for completeness of definite programs. We also study preserving completeness under some cases of pruning of SLD-trees (e.g. due to using the cut). We treat logic programming as a declarative paradigm, abstracting from any operational semantics as far as possible. We argue that the proposed methods are simple enough to be applied, possibly at an informal level, in practical Prolog programming. We point out importance of approximate specifications.</p></details> | 20 pages |
| **[Verifying Array Manipulating Programs with Full-Program Induction](https://arxiv.org/pdf/2002.09857v1)** | 2020-02-25 | <details><summary>Show</summary><p>We present a full-program induction technique for proving (a sub-class of) quantified as well as quantifier-free properties of programs manipulating arrays of parametric size N. Instead of inducting over individual loops, our technique inducts over the entire program (possibly containing multiple loops) directly via the program parameter N. Significantly, this does not require generation or use of loop-specific invariants. We have developed a prototype tool Vajra to assess the efficacy of our technique. We demonstrate the performance of Vajra vis-a-vis several state-of-the-art tools on a set of array manipulating benchmarks.</p></details> |  |
| **[Transformations of CCP programs](https://arxiv.org/pdf/cs/0107014v1)** | 2005-09-17 | <details><summary>Show</summary><p>We introduce a transformation system for concurrent constraint programming (CCP). We define suitable applicability conditions for the transformations which guarantee that the input/output CCP semantics is preserved also when distinguishing deadlocked computations from successful ones and when considering intermediate results of (possibly) non-terminating computations. The system allows us to optimize CCP programs while preserving their intended meaning: In addition to the usual benefits that one has for sequential declarative languages, the transformation of concurrent programs can also lead to the elimination of communication channels and of synchronization points, to the transformation of non-deterministic computations into deterministic ones, and to the crucial saving of computational space. Furthermore, since the transformation system preserves the deadlock behavior of programs, it can be used for proving deadlock freeness of a given program wrt a class of queries. To this aim it is sometimes sufficient to apply our transformations and to specialize the resulting program wrt the given queries in such a way that the obtained program is trivially deadlock free.</p></details> | <details><summary>To ap...</summary><p>To appear in ACM TOPLAS</p></details> |
| **[Constraint Programming viewed as Rule-based Programming](https://arxiv.org/pdf/cs/0003076v2)** | 2005-09-17 | <details><summary>Show</summary><p>We study here a natural situation when constraint programming can be entirely reduced to rule-based programming. To this end we explain first how one can compute on constraint satisfaction problems using rules represented by simple first-order formulas. Then we consider constraint satisfaction problems that are based on predefined, explicitly given constraints. To solve them we first derive rules from these explicitly given constraints and limit the computation process to a repeated application of these rules, combined with labeling.We consider here two types of rules. The first type, that we call equality rules, leads to a new notion of local consistency, called {\em rule consistency} that turns out to be weaker than arc consistency for constraints of arbitrary arity (called hyper-arc consistency in \cite{MS98b}). For Boolean constraints rule consistency coincides with the closure under the well-known propagation rules for Boolean constraints. The second type of rules, that we call membership rules, yields a rule-based characterization of arc consistency. To show feasibility of this rule-based approach to constraint programming we show how both types of rules can be automatically generated, as {\tt CHR} rules of \cite{fruhwirth-constraint-95}. This yields an implementation of this approach to programming by means of constraint logic programming. We illustrate the usefulness of this approach to constraint programming by discussing various examples, including Boolean constraints, two typical examples of many valued logics, constraints dealing with Waltz's language for describing polyhedral scenes, and Allen's qualitative approach to temporal logic.</p></details> | <details><summary>39 pa...</summary><p>39 pages. To appear in Theory and Practice of Logic Programming Journal</p></details> |
| **[Unfolding for CHR programs](https://arxiv.org/pdf/1307.0679v1)** | 2020-02-19 | <details><summary>Show</summary><p>Program transformation is an appealing technique which allows to improve run-time efficiency, space-consumption, and more generally to optimize a given program. Essentially, it consists of a sequence of syntactic program manipulations which preserves some kind of semantic equivalence. Unfolding is one of the basic operations which is used by most program transformation systems and which consists in the replacement of a procedure call by its definition. While there is a large body of literature on transformation and unfolding of sequential programs, very few papers have addressed this issue for concurrent languages. This paper defines an unfolding system for CHR programs. We define an unfolding rule, show its correctness and discuss some conditions which can be used to delete an unfolded rule while preserving the program meaning. We also prove that, under some suitable conditions, confluence and termination are preserved by the above transformation. To appear in Theory and Practice of Logic Programming (TPLP)</p></details> | 49 pages |
| **[Personalized Programming Guidance based on Deep Programming Learning Style Capturing](https://arxiv.org/pdf/2403.14638v1)** | 2024-03-25 | <details><summary>Show</summary><p>With the rapid development of big data and AI technology, programming is in high demand and has become an essential skill for students. Meanwhile, researchers also focus on boosting the online judging system's guidance ability to reduce students' dropout rates. Previous studies mainly targeted at enhancing learner engagement on online platforms by providing personalized recommendations. However, two significant challenges still need to be addressed in programming: C1) how to recognize complex programming behaviors; C2) how to capture intrinsic learning patterns that align with the actual learning process. To fill these gaps, in this paper, we propose a novel model called Programming Exercise Recommender with Learning Style (PERS), which simulates learners' intricate programming behaviors. Specifically, since programming is an iterative and trial-and-error process, we first introduce a positional encoding and a differentiating module to capture the changes of consecutive code submissions (which addresses C1). To better profile programming behaviors, we extend the Felder-Silverman learning style model, a classical pedagogical theory, to perceive intrinsic programming patterns. Based on this, we align three latent vectors to record and update programming ability, processing style, and understanding style, respectively (which addresses C2). We perform extensive experiments on two real-world datasets to verify the rationality of modeling programming learning styles and the effectiveness of PERS for personalized programming guidance.</p></details> | <details><summary>18th ...</summary><p>18th International Conference on Computer Science & Education</p></details> |

